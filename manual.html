<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.5.54" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="Murray Logan" />
<meta name="dcterms.date" content="2024-11-07" />

<title>Darwin Harbour sediment monitoring program analysis application manual</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>


<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="resources/style.css" />
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <div id="quarto-toc-target"></div>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Darwin Harbour sediment monitoring program analysis application manual</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Murray Logan </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">11/07/2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#about" id="toc-about"><span class="header-section-number">1</span> About</a></li>
  <li><a href="#structural-overview" id="toc-structural-overview"><span class="header-section-number">2</span> Structural overview</a>
  <ul>
  <li><a href="#docker-containers" id="toc-docker-containers"><span class="header-section-number">2.1</span> Docker containers</a></li>
  <li><a href="#shiny-applications" id="toc-shiny-applications"><span class="header-section-number">2.2</span> Shiny applications</a></li>
  <li><a href="#git-and-github" id="toc-git-and-github"><span class="header-section-number">2.3</span> Git and github</a></li>
  </ul></li>
  <li><a href="#installation" id="toc-installation"><span class="header-section-number">3</span> Installation</a>
  <ul>
  <li><a href="#installing-docker-desktop" id="toc-installing-docker-desktop"><span class="header-section-number">3.1</span> Installing docker desktop</a>
  <ul>
  <li><a href="#windows" id="toc-windows"><span class="header-section-number">3.1.1</span> Windows</a></li>
  </ul></li>
  <li><a href="#installing-the-and-running-the-app" id="toc-installing-the-and-running-the-app"><span class="header-section-number">3.2</span> Installing the and running the app</a></li>
  </ul></li>
  <li><a href="#the-darwin-harbour-sediment-monitoring-program-analysis-app" id="toc-the-darwin-harbour-sediment-monitoring-program-analysis-app"><span class="header-section-number">4</span> The Darwin Harbour Sediment Monitoring Program Analysis App</a></li>
  <li><a href="#analysis-stages" id="toc-analysis-stages"><span class="header-section-number">5</span> Analysis stages</a>
  <ul>
  <li><a href="#stage-2---obtaining-the-data" id="toc-stage-2---obtaining-the-data"><span class="header-section-number">5.1</span> Stage 2 - obtaining the data</a>
  <ul>
  <li><a href="#read-input-info" id="toc-read-input-info"><span class="header-section-number">5.1.1</span> Read input info</a></li>
  <li><a href="#read-input-data" id="toc-read-input-data"><span class="header-section-number">5.1.2</span> Read input data</a></li>
  <li><a href="#fix-dates" id="toc-fix-dates"><span class="header-section-number">5.1.3</span> Fix dates</a></li>
  <li><a href="#validate-input-data" id="toc-validate-input-data"><span class="header-section-number">5.1.4</span> Validate input data</a></li>
  <li><a href="#make-spatial-data" id="toc-make-spatial-data"><span class="header-section-number">5.1.5</span> Make spatial data</a></li>
  <li><a href="#make-spatial-lookup" id="toc-make-spatial-lookup"><span class="header-section-number">5.1.6</span> Make spatial lookup</a></li>
  </ul></li>
  <li><a href="#stage-3---processing-the-data" id="toc-stage-3---processing-the-data"><span class="header-section-number">5.2</span> Stage 3 - processing the data</a>
  <ul>
  <li><a href="#retrieve-data" id="toc-retrieve-data"><span class="header-section-number">5.2.1</span> Retrieve data</a></li>
  <li><a href="#apply-lors" id="toc-apply-lors"><span class="header-section-number">5.2.2</span> Apply LoRs</a></li>
  <li><a href="#pivot-data" id="toc-pivot-data"><span class="header-section-number">5.2.3</span> Pivot data</a></li>
  <li><a href="#join-metadata" id="toc-join-metadata"><span class="header-section-number">5.2.4</span> Join metadata</a></li>
  <li><a href="#make-sample-key" id="toc-make-sample-key"><span class="header-section-number">5.2.5</span> Make sample key</a></li>
  <li><a href="#collate-data" id="toc-collate-data"><span class="header-section-number">5.2.6</span> Collate data</a></li>
  <li><a href="#incorporate-spatial-data" id="toc-incorporate-spatial-data"><span class="header-section-number">5.2.7</span> Incorporate spatial data</a></li>
  <li><a href="#tidy-data" id="toc-tidy-data"><span class="header-section-number">5.2.8</span> Tidy data</a></li>
  <li><a href="#standardise-data" id="toc-standardise-data"><span class="header-section-number">5.2.9</span> Standardise data</a></li>
  <li><a href="#create-site-lookup" id="toc-create-site-lookup"><span class="header-section-number">5.2.10</span> Create site lookup</a></li>
  </ul></li>
  <li><a href="#stage-4---exploratory-data-analysis" id="toc-stage-4---exploratory-data-analysis"><span class="header-section-number">5.3</span> Stage 4 - Exploratory data analysis</a></li>
  <li><a href="#stage-5---temporal-analysis" id="toc-stage-5---temporal-analysis"><span class="header-section-number">5.4</span> Stage 5 - Temporal analysis</a>
  <ul>
  <li><a href="#retrieve-data-1" id="toc-retrieve-data-1"><span class="header-section-number">5.4.1</span> Retrieve data</a></li>
  <li><a href="#prepare-data" id="toc-prepare-data"><span class="header-section-number">5.4.2</span> Prepare data</a></li>
  <li><a href="#prepare-priors" id="toc-prepare-priors"><span class="header-section-number">5.4.3</span> Prepare priors</a></li>
  <li><a href="#prepare-model-template" id="toc-prepare-model-template"><span class="header-section-number">5.4.4</span> Prepare model template</a></li>
  <li><a href="#fit-models" id="toc-fit-models"><span class="header-section-number">5.4.5</span> Fit models</a></li>
  <li><a href="#validate-models" id="toc-validate-models"><span class="header-section-number">5.4.6</span> Validate models</a></li>
  <li><a href="#compile-zone-results" id="toc-compile-zone-results"><span class="header-section-number">5.4.7</span> Compile zone results</a></li>
  <li><a href="#collect-zone-results" id="toc-collect-zone-results"><span class="header-section-number">5.4.8</span> Collect zone results</a></li>
  <li><a href="#compile-site-results" id="toc-compile-site-results"><span class="header-section-number">5.4.9</span> Compile site results</a></li>
  <li><a href="#collect-site-results" id="toc-collect-site-results"><span class="header-section-number">5.4.10</span> Collect site results</a></li>
  <li><a href="#compile-area-results" id="toc-compile-area-results"><span class="header-section-number">5.4.11</span> Compile area results</a></li>
  <li><a href="#collect-area-results" id="toc-collect-area-results"><span class="header-section-number">5.4.12</span> Collect area results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#app-pages" id="toc-app-pages"><span class="header-section-number">6</span> App pages</a>
  <ul>
  <li><a href="#sec-landing" id="toc-sec-landing"><span class="header-section-number">6.1</span> Landing page</a></li>
  <li><a href="#sec-dashboard" id="toc-sec-dashboard"><span class="header-section-number">6.2</span> Dashboard</a></li>
  <li><a href="#sec-data" id="toc-sec-data"><span class="header-section-number">6.3</span> Data</a>
  <ul>
  <li><a href="#raw-data-panel" id="toc-raw-data-panel"><span class="header-section-number">6.3.1</span> Raw data panel</a></li>
  <li><a href="#processed-data-panel" id="toc-processed-data-panel"><span class="header-section-number">6.3.2</span> Processed data panel</a></li>
  </ul></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis"><span class="header-section-number">6.4</span> Exploratory Data Analysis</a>
  <ul>
  <li><a href="#temporal" id="toc-temporal"><span class="header-section-number">6.4.1</span> Temporal</a></li>
  <li><a href="#temporal-type" id="toc-temporal-type"><span class="header-section-number">6.4.2</span> Temporal Type</a></li>
  <li><a href="#temporal-site" id="toc-temporal-site"><span class="header-section-number">6.4.3</span> Temporal Site</a></li>
  <li><a href="#spatial-type" id="toc-spatial-type"><span class="header-section-number">6.4.4</span> Spatial Type</a></li>
  </ul></li>
  <li><a href="#analysis" id="toc-analysis"><span class="header-section-number">6.5</span> Analysis</a>
  <ul>
  <li><a href="#analysis-overview" id="toc-analysis-overview"><span class="header-section-number">6.5.1</span> Analysis overview</a></li>
  <li><a href="#model-diagnostics" id="toc-model-diagnostics"><span class="header-section-number">6.5.2</span> Model diagnostics</a></li>
  <li><a href="#analysis-details" id="toc-analysis-details"><span class="header-section-number">6.5.3</span> Analysis details</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
<section id="about" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> About</h1>
<p>This document comprises the manual for the Darwin Harbour sediment monitoring program analysis application. It provides information on:</p>
<ul>
<li>a broad overview of the structure of the application</li>
<li>the application dependencies and how to install them</li>
<li>starting the application</li>
<li>progressing through the analysis pipeline</li>
<li>visualising, interpreting and extracting outputs</li>
</ul>
</section>
<section id="structural-overview" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Structural overview</h1>
<p><a href="https://www.r-project.org/">R Graphical and Statistical Environment</a> offers an ideal platform for developing and running complex statistical analyses as well as presenting the outcomes via professional graphical/tabular representations. As a completely scripted language it also offers the potential for both full transparency and reproducibility. Nevertheless, as the language, and more specifically the extension packages are community developed and maintained, the environment evolves over time. Similarly, the underlying operating systems and programs on which R and its extension packages depend (hereafter referred to as the <em>operating environment</em>) also change over time. Consequently, the stability and reproducibility of R codes have a tendency to change over time.</p>
<section id="docker-containers" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Docker containers</h2>
<p>One way to attempt to future proof a codebase that must be run upon a potentially unpredictable operating environment is to <strong>containerise</strong> the operating environment, such that it is preserved to remain unchanged over time. Containers (specifically <a href="https://www.docker.com/">docker</a> containers) are lightweight abstraction units that encapsulate applications and their dependencies within standardized, self-contained execution environments. Leveraging containerization technology, they package application code, runtime, libraries, and system tools into isolated units (<em>containers</em>) that abstract away underlying infrastructure differences, enabling consistent and predictable execution across diverse computing platforms.</p>
<p>Containers offer several advantages, such as efficient resource utilization, rapid deployment, and scalability. They enable developers to build, test, and deploy applications with greater speed and flexibility. Docker containers have become a fundamental building block in modern software development, enabling the development and deployment of applications in a consistent and predictable manner across various environments.</p>
</section>
<section id="shiny-applications" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Shiny applications</h2>
<p><a href="https://shiny.posit.co/">Shiny</a> is a web application framework for R that enables the creation of interactive and data-driven web applications directly from R scripts. Developed by <a href="https://posit.co/">Rstudio</a>, Shiny simplifies the process of turning analyses into interactive web-based tools without the need for extensive web development expertise.</p>
<p>What makes Shiny particularly valuable is its seamless integration with R, allowing statisticians and data scientists to build and deploy bespoke statistical applications, thereby making data visualization, exploration, and analysis accessible to a broader audience. With its interactive and user-friendly nature, Shiny serves as a powerful tool for sharing insights and engaging stakeholders in a more intuitive and visual manner.</p>
</section>
<section id="git-and-github" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Git and github</h2>
<p>Git, a distributed version control system, and <a href="https://github.com/">GitHub</a>, a web-based platform for hosting and collaborating on Git repositories, play pivotal roles in enhancing reproducibility and transparency in software development. By tracking changes in source code and providing a centralized platform for collaborative work, Git and GitHub enable developers to maintain a detailed history of code alterations. This history serves as a valuable asset for ensuring the reproducibility of software projects, allowing users to trace and replicate specific versions of the codebase.</p>
<p>GitHub Actions (an integrated workflow automation feature of GitHub), automates tasks such as building, testing, and deploying applications and artifacts. Notably, through workflow actions, GitHub Actions can build docker containers and act as a container registry. This integration enhances the overall transparency of software development workflows, making it easier to share, understand, and reproduce projects collaboratively.</p>
<p><a href="#fig-diagram" class="quarto-xref">Figure 1</a> provides a schematic overview of the relationship between the code produced by the developer, the Github cloud repositiory and container registry and the shiny docker container run by user.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="manual_files/figure-html/fig-diagram-1.png" class="img-fluid" width="576" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<strong>Figure</strong> 1: Diagram illustrating the relationship between the code produced by the developer and the shiny docker container utilised by user with a Github cloud conduit. The developed codebase includes a Shiny R application with R backend, <code>Dockerfile</code> (instructions used to assemble a full operating environment) and github workflow file (instructions for building and packaging the docker image on github via <code>actions</code>).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="installation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Installation</h1>
<section id="installing-docker-desktop" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Installing docker desktop</h2>
<p>To retrieve and run docker containers requires the installation of <a href="https://www.docker.com/products/docker-desktop/">Docker Desktop</a> on Windows and MacOSx</p>
<section id="windows" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1"><span class="header-section-number">3.1.1</span> Windows</h3>
<p>The steps for installing Docker Desktop are:</p>
<ul>
<li><p><strong>Download the Installer:</strong> head to <a href="https://docs.docker.com/desktop/install/windows-install/" class="uri">https://docs.docker.com/desktop/install/windows-install/</a> and follow the instructions for downloading the appropriate installer for your Windows version (Home or Pro).</p></li>
<li><p><strong>Run the Installer:</strong> double-click the downloaded file and follow the on-screen instructions from the installation wizard. Accept the license agreement and choose your preferred installation location.</p></li>
<li><p><strong>Configure Resources (Optional):</strong> Docker Desktop might suggest allocating some system resources like CPU and memory. These settings can be adjusted later, so feel free to use the defaults for now.</p></li>
<li><p><strong>Start the Docker Engine:</strong> once installed, click the “Start Docker Desktop” button. You may see a notification in the taskbar - click it to confirm and allow Docker to run in the background.</p></li>
<li><p><strong>Verification:</strong> open a terminal (or Powershell) and run <code>docker --version</code>. If all went well, you should see information about the installed Docker Engine version.</p></li>
</ul>
<p>Additional Tips:</p>
<ul>
<li>Ensure Hyper-V (virtualization) is enabled in your BIOS settings for optimal performance.</li>
</ul>
</section>
</section>
<section id="installing-the-and-running-the-app" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Installing the and running the app</h2>
<p>The task of installing and running the app is performed via a single <strong>deploy script</strong> (<code>deploy.bat</code> on Windows or <code>deploy.sh</code> on Linux/MacOSX/wsl). For this to work properly, the deploy script should be placed in a folder along with a folder (called <code>input</code>) that contains the input datasets (in excel format). This structure is illustrated below for Windows.</p>
<pre><code>\
|- deploy.bat
|- input
   |- dataset1.xlsx
   |- dataset2.xlsx</code></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above illustration, there are two example datasets (<code>dataset1.xlsx</code> and <code>dataset2.xlsx</code>). The datasets need NOT be called <code>dataset1.xlsx</code>. They can have any name you choose, so long as they are excel files that adhere to the structure outlined in <a href="#sec-data-requirements" class="quarto-xref">Section 5.1.4.1</a>.</p>
</div>
</div>
<p>To set up the above struture:</p>
<ol type="1">
<li><p>create a new folder on your computer in a location of your choice that you are likely to remember and easily locate (e.g. on the desktop). Whilst the name of the folder is not important, it is recommended that it be named after the project (e.g. <code>darwin_harbour_sediment_monitoring</code>).</p></li>
<li><p>download the deploy script from the projects github repository</p>
<ol type="a">
<li><p>go to the projects github repository (<a href="https://github.com/open-AIMS/dh_sediment_monitoring.git" class="uri">https://github.com/open-AIMS/dh_sediment_monitoring.git</a>) in a browser</p></li>
<li><p>click on either the <code>deploy.bat</code> (Windows) or ’deploy.sh` (Linux/MacOSX/wsl).</p>
<p><img src="resources/github_deploy_script.png" class="img-fluid" /></p></li>
<li><p>click on the download button and select the project folder as the location to download the file to. If the file is automatically downloaded to a downloads folder, move the file to the project folder.</p>
<p><img src="resources/github_deploy_script2.png" class="img-fluid" /></p></li>
</ol></li>
<li><p>within the project folder, create a folder called <code>inputs</code> and place all the appropriate data sets into this <code>inputs</code> folder</p></li>
</ol>
<p>To run the app, navigate inside of the project folder and run (typically double click) on the deploy script. Upon doing so, you will be presented with a directory selection window that is prompting for the path of the project folder. Navigate to and select the project folder before clicking the “OK” button. Shortly thereafter, the application will appear in a browser tab.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
More specific information about the <code>deploy.bat</code> script
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The <code>deploy.bat</code> script performs the following:</p>
<ol type="1">
<li>defines paths to the project repository and local project folder</li>
<li>checks if <code>docker</code> is installed and available from the command line for the current user</li>
<li>checks if <code>docker</code> is running</li>
<li>query the user for the location of the project folder</li>
<li>determine whether there are any updates to the <code>docker</code> image and if so pull them down</li>
<li>run the <code>docker</code> container</li>
<li>open the shiny app in a browser</li>
</ol>
</div>
</div>
</div>
</section>
</section>
<section id="the-darwin-harbour-sediment-monitoring-program-analysis-app" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Darwin Harbour Sediment Monitoring Program Analysis App</h1>
<p>This <a href="https://shiny.posit.co/">Shiny</a> application is designed to ingest very specifically structured excel spreadsheets containing Darwin Harbour sediment monitoring data and produce various analyses and visualisations. The application is served from a <a href="https://www.docker.com/">docker</a> container to the localhost and the default web browser.</p>
<p>Docker containers can be thought of a computers running within other computers. More specifically, a container runs an instance of image built using a series of specific instructions that govern the entire software environment. As a result, containers run from the same image will operate (virtually) identically regardless of the host environment. Furthermore, since the build instructions can specify exact versions of all software components, containers provide a way of maximising the chances that an application will continue to run as designed into the future despite changes to operating environments and dependencies.</p>
<p>This shiny application comprises five pages (each accessable via the sidebar menu on the left side of the screen):</p>
<ol type="1">
<li>a <strong>Landing</strong> page (this page) providing access to the settings and overall initial instructions</li>
<li>a <strong>Dashboard</strong> providing information about the progression of tasks in the analysis pipeline</li>
<li>a <strong>Data</strong> page providing overviews of data in various stages</li>
<li>an <strong>Exploratory Data Analysis</strong> page providing graphical data summaries</li>
<li>an <strong>Analysis</strong> page providing information about the statistical models and their outputs</li>
<li>a <strong>Manual</strong> page that displays the online manual for the application</li>
</ol>
<p>Each page will also contain instructions to help guide you through using or interpreting the information. In some cases, this will take the from of an info box (such as the current box). In other cases, it will take the form of little <span class="fas fa-circle-info"></span> symbols whose content is revealed with a mouse hover.</p>
<p>There are numerous stages throughout the analysis pipeline that may require user review (for example examining the exploratory data analysis figures to confirm that the data are as expected). Consequently, it is necessary for the user to manually trigger each successive stage of the pipeline. The stages are:</p>
<ul>
<li><p>Stage 1 - Prepare environment</p>
<details>
<summary>
<p>More info</p>
</summary>
<p class="details-info">
<p>This stage is run automatically on startup and essentially sets up the operating environment.</p>
</p>
</details></li>
<li><p>Stage 2 - Obtain data</p>
<details>
<summary>
<p>More info</p>
</summary>
<p class="details-info">
<p>This stage comprises of the following steps:</p>
<ul>
<li>reading in the excel files within the nominated input path</li>
<li>validating the input data according to a set of validation rules</li>
<li>constructing various spatial objects for mapping and spatial aggregation purposes</li>
</ul>
<p>The tables within the <strong>Raw data</strong> tab of the <strong>Data</strong> page will also be populated.</p>
</p>
</details></li>
<li><p>Stage 3 - Process data</p>
<details>
<summary>
<p>More info</p>
</summary>
<p class="details-info">
<p>This stage comprises of the following steps:</p>
<ul>
<li>apply limit of reporing values (LoRs)</li>
<li>pivot the data into a longer format that is more suitable for analysis and graphing</li>
<li>join in the metadata to each associated sheet</li>
<li>make a unique key</li>
<li>collate the all the data together from across the multiple sheets and files into a single data set</li>
<li>incorporate the spatial data</li>
<li>tidy the field names</li>
<li>apply data standardisations</li>
<li>create a site lookup table to facilitate fast incorporation of spatial information into any outputs.</li>
</ul>
<p>The tables within the <strong>Processed data</strong> tab of the <strong>Data</strong> page will also be populated.</p>
</p>
</details></li>
<li><p>Stage 4 - Exploratory data analysis</p>
<details>
<summary>
<p>More info</p>
</summary>
<p class="details-info">
<p>This stage comprises of the following steps:</p>
<ul>
<li>retrieve the processed data.</li>
<li>construct spatio-temporal design plots conditioned on initial sampling semester</li>
<li>construct variable temporal design plots conditioned on harbour zone</li>
<li>construct site level temporal trends for each variable</li>
<li>construct zone level temporal and spatial visualisations for each variable</li>
</ul>
<p>The exploratory data figures of the <strong>Exploratory Data Analysis</strong> page will also be populated.</p>
</p>
</details></li>
<li><p>Stage 5 - Temporal analyses</p>
<details>
<summary>
<p>More info</p>
</summary>
<p class="details-info">
<p>This stage comprises of the following steps:</p>
<ul>
<li>retrieve the processed data</li>
<li>prepare the data for modelling</li>
<li>prepare appropriate model formulae for each zone, variable, standardisation type</li>
<li>prepare appropriate model priors for each zone, variable, standardisation type</li>
<li>prepare appropriate model template</li>
<li>fit the models for each zone, variable, standardisation type</li>
<li>perform model validations for each zone, variable, standardisation type</li>
<li>estimate all the contrasts for each model and collate all the effects</li>
</ul>
</p>
</details></li>
</ul>
<p>Underneath the sidebar menu there are a series of buttons that control progression through the analysis pipeline stages. When a button is blue (and has a play icon), it indicates that the Stage is the next Stage to be run in the pipeline. Once a stage has run, the button will turn green. Grey buttons are disabled.</p>
<p>Clicking on button will run that stage. Once a stage has run, the button will change to either green (success), yellow (orange) or red (failures) indicating whether errors/warnings were encountered or not. If the stage was completed successfully, the button corresponding to the next available stage will be activated.</p>
<p>Sidebar menu items that are in orange font are active and clicking on an active menu item will reveal an associated page. Inactive menu items are in grey font. Menu items will only become active once the appropriate run stage has been met. The following table lists the events that activate a menu item.</p>
<div class="table-minimal">
<table class="caption-top">
<thead>
<tr class="header">
<th>Menu Item</th>
<th>Trigger Event</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Landing</td>
<td>Always active</td>
</tr>
<tr class="even">
<td>Dashboard</td>
<td>Always active</td>
</tr>
<tr class="odd">
<td>Data</td>
<td>After Stage 2</td>
</tr>
<tr class="even">
<td>Exploratory Data Analysis</td>
<td>After Stage 4</td>
</tr>
<tr class="odd">
<td>Analysis</td>
<td>After Stage 5</td>
</tr>
<tr class="even">
<td>Manual</td>
<td>Always active</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#fig-diagram2" class="quarto-xref">Figure 2</a> provides a schematic overview the sequence of filesystem events that occur during the development, deployment and running of this app.</p>
<ol type="1">
<li>the developed codebase is pushed to github and if necessary continuous integration (github actions) is triggered. The continuous integration will re-build and host a docker image as well as rebuild the manual.</li>
<li>when the client runs the <code>deploy.bat</code> (or <code>deploy.sh</code>) script, it will check whether docker is running and get input from the user about the location of the project directory.</li>
<li>github will be queried to discover if a new docker image is available. If so, then the new image will be pulled down locally and run (if docker is runnning).</li>
<li>the docker container will be run and this will trigger git within the container to pull down the latest version of the codebase from github to a temporary repo in the container. As the container is starting up, it will mount the project folder so that its contents are available to the environment within container and outputs produced within the container are available to the host.</li>
<li>some of the files in the temporary repo will be copied to a folder within the project folder.</li>
<li>the shiny app will start up on <code>port 3838</code> of the localhost and this will be offered to the default browser.</li>
<li>as the shiny app progresses through each of the analysis stages, more data will be added to various folders of the project directory.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagram2" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-diagram2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="manual_files/figure-html/fig-diagram2-1.png" class="img-fluid" width="576" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagram2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<strong>Figure</strong> 2: Diagram illustrating the sequence of filesystem events that occur during the development, deployment and running of this app.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="analysis-stages" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Analysis stages</h1>
<section id="stage-2---obtaining-the-data" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Stage 2 - obtaining the data</h2>
<p>At the completion of this stage, the Data sidebar menu and Stage 3 button will become active and the Data page will be populated with the raw data and available for review.</p>
<section id="read-input-info" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1"><span class="header-section-number">5.1.1</span> Read input info</h3>
<p>This task seeks to determine what data sources are available and for those found, stores the names and filetypes discovered. This task will exclusively search in the <code>/input</code> folder of the project directory.</p>
</section>
<section id="read-input-data" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2"><span class="header-section-number">5.1.2</span> Read input data</h3>
<p>This task will sequentially read in each sheet of each data source (excel file) into a nested list.</p>
</section>
<section id="fix-dates" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3"><span class="header-section-number">5.1.3</span> Fix dates</h3>
<p>This task will ensure that all dates are of the same format. Spreadsheets often store date/time data in one format and display it in another format. Consequently, users can be unaware that they have a mixture of date/time formats present in the same spreadsheet. For the purpose of data analysis, it is important that all date/time formats are consistent - this task aims to achieve this.</p>
</section>
<section id="validate-input-data" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4"><span class="header-section-number">5.1.4</span> Validate input data</h3>
<p>This task performs data validation in accordance with the rules set out in the following section.</p>
<section id="sec-data-requirements" class="level4">
<h4>Data requirements</h4>
<p>To be valid, input data must be excel files (*.xlsx) comprising at least the following sheets (each of which must at least have the fields listed in their respective tables):</p>
<ul>
<li><p>metals</p>
<div class="table-minimal">
<table class="caption-top">
<colgroup>
<col style="width: 10%" />
<col style="width: 44%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Description</th>
<th>Validation conditions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample_ID</td>
<td>unique sample ID</td>
<td>must contain characters</td>
</tr>
<tr class="even">
<td>*¹ (mg/kg)</td>
<td>observed concentration of metal in sediment sample</td>
<td>must contain only numbers or start with a ‘&lt;’ symbol</td>
</tr>
</tbody>
</table>
<p>1: where the ’*’ represents a one or two character chemical symbol (such as ‘Ag’ or ‘V’). There should be numerous of these fields</p>
</div></li>
<li><p>hydrocarbons</p>
<div class="table-minimal">
<table class="caption-top">
<colgroup>
<col style="width: 10%" />
<col style="width: 50%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Description</th>
<th>Validation conditions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample_ID</td>
<td>unique sample ID</td>
<td>must contain characters</td>
</tr>
<tr class="even">
<td>&gt;C*¹</td>
<td>observed concentration of hydrocarbons within a specific size bin in sediment sample</td>
<td>must contain only numbers or start with a ‘&lt;’ symbol</td>
</tr>
</tbody>
</table>
1: where the ’*’ represents a string of characters defining the size bin (such as ’10 _C16’). There should be numerous of these fields
</div></li>
<li><p>total_carbons</p>
<div class="table-minimal">
<table class="caption-top">
<colgroup>
<col style="width: 10%" />
<col style="width: 50%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Description</th>
<th>Validation conditions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample_ID</td>
<td>unique sample ID</td>
<td>must contain characters</td>
</tr>
<tr class="even">
<td>TOC (%)</td>
<td>observed total organic carbon (as a percentage of the sample weight)</td>
<td>must contain only numbers</td>
</tr>
</tbody>
</table>
</div></li>
<li><p>metadata</p>
<div class="table-minimal">
<table class="caption-top">
<colgroup>
<col style="width: 25%" />
<col style="width: 35%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Description</th>
<th>Validation conditions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>IBSM_site</td>
<td>name of the site from the perspective of IBSM</td>
<td>must contain characters (or be blank)</td>
</tr>
<tr class="even">
<td>Site_ID</td>
<td>a unique site ID</td>
<td>must contain characters (cannot be blank)</td>
</tr>
<tr class="odd">
<td>Sample_ID</td>
<td>unique sample ID (the key to data sheets)</td>
<td>must contain characters (cannot be blank)</td>
</tr>
<tr class="even">
<td>Original_SampleID</td>
<td>unique sample ID</td>
<td>must contain characters</td>
</tr>
<tr class="odd">
<td>Latitude</td>
<td>site latitude</td>
<td>must be numeric (and negative)</td>
</tr>
<tr class="even">
<td>Longitude</td>
<td>site longitude</td>
<td>must be numeric</td>
</tr>
<tr class="odd">
<td>Acquire_date_time</td>
<td>date and time sample was collected (D/M/YYYY hh:mm:ss)</td>
<td>must be in datetime format</td>
</tr>
<tr class="even">
<td>Sampler</td>
<td>name of person responsible for collecting sample (ignored)</td>
<td>ignored</td>
</tr>
<tr class="odd">
<td>Notes</td>
<td>project description (ignored)</td>
<td>ignored</td>
</tr>
<tr class="even">
<td>Baseline_site</td>
<td>the unique site ID of the corresponding baseline sample</td>
<td>must contain characters (cannot be blank)</td>
</tr>
<tr class="odd">
<td>Baseline_acquire_date_site</td>
<td>the date and time of the corresponding baseline sample</td>
<td>must be in datetime format</td>
</tr>
</tbody>
</table>
</div></li>
<li><p>notes - this sheet is not processed or validated</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>All input data must be placed in the <code>/input</code> folder of the Project prior to starting the app.</p>
</div>
</div>
</section>
</section>
<section id="make-spatial-data" class="level3" data-number="5.1.5">
<h3 data-number="5.1.5"><span class="header-section-number">5.1.5</span> Make spatial data</h3>
<p>This task will compile a set of spatial artifacts from GIS shapefiles of Darwin Harbour. These spatial artifacts will be used to spatially join the sediment data in order to assign spatial scales such as Zones and Areas to the data. They will also be used to facilitate mapping of the data. The shapefiles used in this task are built into the app. If there is a need to change these, please contact the app author.</p>
</section>
<section id="make-spatial-lookup" class="level3" data-number="5.1.6">
<h3 data-number="5.1.6"><span class="header-section-number">5.1.6</span> Make spatial lookup</h3>
<p>This stage creates a lookup table that relates each of the spatial scales to one another. This lookup is used to inject the spatial information into the data and modelled derivatives after they are created and in so doing prevents the need to spatially join the data each time it is required.</p>
</section>
</section>
<section id="stage-3---processing-the-data" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span> Stage 3 - processing the data</h2>
<p>At the completion of this stage, the Stage 4 button will become active and the Processed Data sub-page of the Data page will be populated with the processed data and available for review.</p>
<section id="retrieve-data" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1"><span class="header-section-number">5.2.1</span> Retrieve data</h3>
<p>This task literally just reads in the data stored at the end of the previous stage.</p>
</section>
<section id="apply-lors" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2"><span class="header-section-number">5.2.2</span> Apply LoRs</h3>
<p>This task applies rules for the presence of data that are below Limit of Reporting (LoR). In the data, LoR values are indicated by the presence of a <code>&lt;</code> symbol. There are two ways available for handling LoR values.</p>
<ol type="1">
<li>Traditionally, values that represent Limit of Reporting (LoR) were replaced with half the LoR value. For example, a value of &lt;0.02 would be replaced with 0.01.</li>
<li>However, modern statistical analyses have more appropriate ways of incorporating LoR information. Rather than arbitrarily replace values with half the LoR, we retain their value and flag them as censored and allow the statistical properties of disbutions to handle them more naturally.</li>
</ol>
<p>In either case, a LoR flag is then attached to any value that was deemed LoR.</p>
</section>
<section id="pivot-data" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3"><span class="header-section-number">5.2.3</span> Pivot data</h3>
<p>This task pivots (reshapes) the data from wide to long format. Wide format, in which each row represents a single site/time and each variable is in its own column is a convenient way to assemble data (particularly as it permits the user to easily identify missing values). However, data analysis requires that each individual record (observation) be in its own row.</p>
</section>
<section id="join-metadata" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4"><span class="header-section-number">5.2.4</span> Join metadata</h3>
<p>This task joins (merges) the each of the main sediment data sheets (metals, hydrocarbons and total carbons) with the metadata sheet.</p>
</section>
<section id="make-sample-key" class="level3" data-number="5.2.5">
<h3 data-number="5.2.5"><span class="header-section-number">5.2.5</span> Make sample key</h3>
<p>This task generates a unique key to uniquely identify each individual record by combining information about the Site_ID, acquire date/time and the part of the sample ID that indicates whether or not the sample was a replicate or duplicate.</p>
</section>
<section id="collate-data" class="level3" data-number="5.2.6">
<h3 data-number="5.2.6"><span class="header-section-number">5.2.6</span> Collate data</h3>
<p>This task combines all the data sources (years) and types (metals, hydrocarbons, total carbons) together into a single data set. At the same time, it also creates some additional fields:</p>
<ul>
<li><code>Year_cal</code> a field that represents the calendar year in which the sample was collected</li>
<li><code>Year_fiscal</code> a field that represents the fiscal year in which the sample was collected</li>
<li><code>Year_water</code> a field that represents the water year (defined as 1st Oct through to 30 Sept) in which the sample was collected</li>
<li><code>Baseline</code> a field that represents whether the observation is considered a “Baseline” observation or not</li>
<li><code>Replicate_flag</code> a field that represents whether the observation is a replicate</li>
<li><code>Duplicate_flag</code> a field that represents whether the observation is a duplicate</li>
</ul>
</section>
<section id="incorporate-spatial-data" class="level3" data-number="5.2.7">
<h3 data-number="5.2.7"><span class="header-section-number">5.2.7</span> Incorporate spatial data</h3>
<p>This task uses the spatial artifacts created in the previous stage to add spatial information to the data. This spatial information includes the Zone, Area and Site that each observation belongs to.</p>
</section>
<section id="tidy-data" class="level3" data-number="5.2.8">
<h3 data-number="5.2.8"><span class="header-section-number">5.2.8</span> Tidy data</h3>
<p>This task creates a new field <code>Site</code> that acts as a unique identifier of a sampling location over time. This field is created by copying the <code>IBSM_site</code> field (if it is not empty), otherwise the <code>Site_ID</code> field is used. This task also removes any fields that are no longer required.</p>
</section>
<section id="standardise-data" class="level3" data-number="5.2.9">
<h3 data-number="5.2.9"><span class="header-section-number">5.2.9</span> Standardise data</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-standardisations" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-standardisations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="manual_files/figure-html/fig-standardisations-1.png" class="img-fluid" width="576" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-standardisations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<strong>Figure</strong> 3: Diagram illustrating the standardisation (normalisation) rules applied to each variable. In each case, the text in blue represents the appropriate divisor used in the standardisation.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="create-site-lookup" class="level3" data-number="5.2.10">
<h3 data-number="5.2.10"><span class="header-section-number">5.2.10</span> Create site lookup</h3>
<p>This task creates a lookup that maps sites to zones.</p>
</section>
</section>
<section id="stage-4---exploratory-data-analysis" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span> Stage 4 - Exploratory data analysis</h2>
<p>At the completion of this stage, the Exploratory Data Analysis menu and Stage 5 button will become active and the Exploratory Data Analysis page will be populated with the a range of exploratory figures. This Stage involves numerous tasks that each prepare the data in formats conducive to the production of the figures while navigating the Exploratory Data Analysis page.</p>
</section>
<section id="stage-5---temporal-analysis" class="level2" data-number="5.4">
<h2 data-number="5.4"><span class="header-section-number">5.4</span> Stage 5 - Temporal analysis</h2>
<p>At the completion of this stage, the Analysis menu will become active and the Analysis page will be populated with the a range of modelled outputs.</p>
<p>The temporal analyses essentially involve the fitting of separate Bayesian Hierarchical models <span class="citation" data-cites="Gelman-2007-2007">(<a href="#ref-Gelman-2007-2007" role="doc-biblioref">Gelman and Hill 2007</a>)</span> to the full time series of all sites within each focal Zone. From such models (outline below), site and zone level modelled trends can be inferred and thereafter aggregated up to Area and Whole of Harbour level trends as well.</p>
<p>The general form of the models employed is as follows:</p>
<p><span class="math display">\[
\begin{aligned}
y_{i,s} &amp;\sim{} \Gamma(\mu_{i,s}, \phi)\\
log(\mu_{i,s}) &amp;= (\beta_0 + \gamma_{s[i],0}) + \sum_{j=1}^nT_{[i],j}.(\beta_j + \gamma_{s[i],j]})\\
\phi&amp;\sim\Gamma(0.01, 0.01)\\
\beta_0&amp;\sim{}\mathit{t}(3, \alpha_1, \alpha_2)\\
\beta_{[1,n]}&amp;\sim{}\mathit{t}(3, 0, \alpha_3)\\
\gamma_{[1..p]}&amp;\sim{}MVN(0, \boldsymbol{\Sigma_s})\\
\boldsymbol{\Sigma_s} &amp;=
{\begin{pmatrix}
\sigma_{s_1}^2 &amp; \rho_s \sigma_{s_1} \sigma_{s_2} &amp; \rho_s \sigma_{s_1} \sigma_{s_3}\\
\rho_s \sigma_{s_1} \sigma_{s_2} &amp; \sigma_{s_2}^2 &amp; \rho_s \sigma_{s_2} \sigma_{s_3}\\
\rho_s \sigma_{s_1} \sigma_{s_3}  &amp; \rho_s \sigma_{s_2} \sigma_{s_3} &amp; \sigma_{s_3}^2
\end{pmatrix}}\\
\sigma_{s[1,2,3]} &amp;\sim \mathit{t}(3, 0, \alpha_2)\\
\rho_s &amp;\sim \mathit{LKJcorr}(1)\\
\end{aligned}
\]</span></p>
<p>The <span class="math inline">\(i_{th}\)</span> value (<span class="math inline">\(y\)</span>) from the <span class="math inline">\(s_{th}\)</span> site was assumed to be drawn from a gamma (<span class="math inline">\(\Gamma\)</span>) distribution parameterised by a mean (<span class="math inline">\(\mu_{i,s}\)</span>) and dispersion (<span class="math inline">\(\phi\)</span>) respectively. The (natural log) expected means were described by a linear model that included an intercept (<span class="math inline">\(\beta_0\)</span>), varying effects of site (<span class="math inline">\(\gamma_{s,0}\)</span>) and annual changes in value (<span class="math inline">\(\gamma_{s,j}\)</span>) as well as the population effects (<span class="math inline">\(\beta_1\)</span>) of year (<span class="math inline">\(T\)</span>). Weakly informative flat-t (3 df) priors were applied to the intercept and all population effect parameters. The values (<span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(alpha_2\)</span> and <span class="math inline">\(\alpha_3\)</span>) used to define the weakly informative priors were developed from simple summary statistics of the raw data. A weakly informative gamma prior was applied to the dispersion parameter. The varying effects were assumed to follow a multivariate normal with a site-specific covariance structure whose variances follow a weakly informative flat t distribution and whose correlation follows a LJK distribution with parameter of 1.</p>
<p>When the data include values that are below the limit of detection, the model outlined above is modified so as to apply left censoring.</p>
<p>All Bayesian models were fit using the <code>brms</code> <span class="citation" data-cites="brms">(<a href="#ref-brms" role="doc-biblioref">Bürkner 2017</a>)</span> package within the R Graphical and Statistical Environment <span class="citation" data-cites="R-2024">(<a href="#ref-R-2024" role="doc-biblioref">R Core Team 2024</a>)</span>. All models had an adaptive delta of 0.95 and had three chains, each with 5000 no-u-turn (NUTS) iterations, a warmup of 1000 and a thinning rate of 5.</p>
<p>Separate models are fit to each variable, for each appropriate standardisation type, for each zone. At the time of writing this manual, this equates to nearly 300 models.</p>
<section id="retrieve-data-1" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1"><span class="header-section-number">5.4.1</span> Retrieve data</h3>
<p>This task literally just reads in the data stored at the end of the previous stage.</p>
</section>
<section id="prepare-data" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2"><span class="header-section-number">5.4.2</span> Prepare data</h3>
<p>This task ensures that the data are formatted and packaged up into sets associated with each individual model.</p>
</section>
<section id="prepare-priors" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3"><span class="header-section-number">5.4.3</span> Prepare priors</h3>
<p>This task is responsible for defining weakly informative priors on all parameters for each model. The priors associated with the model for a specific variable/zone were developed by taking simple summary statistics of the mean, median, standard deviation and median absolute deviation of the log of the values conditional on year.</p>
<ul>
<li><span class="math inline">\(\alpha_1\)</span> was taken from the median of the log values from the first sampling year. This was used as the mean of the model intercept (<span class="math inline">\(\beta_0\)</span>) prior</li>
<li><span class="math inline">\(\alpha_2\)</span> was taken from the maximum of the median absolute deviations of log values for each sampling year. This was used as the standard deviation for the model intercept (<span class="math inline">\(\beta_0\)</span>) as well as the standard deviation for the variances (<span class="math inline">\(\sigma_s\)</span>) of the varying effects.</li>
<li><span class="math inline">\(\alpha_3\)</span> was taken as the maximum of the difference in mean log values between years and this was used to inform the standard deviation of the population effect (<span class="math inline">\(\beta\)</span>) priors.</li>
</ul>
</section>
<section id="prepare-model-template" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4"><span class="header-section-number">5.4.4</span> Prepare model template</h3>
<p>This task essentially involves compiling a single simple model to use as a template for most other models. Model compilation consumes approximately 40 seconds of time prior to the model running. Since most of the models are the same (only the priors and the data differ), and this project requires the fitting of a very large number of models, the use of a pre-compiled template can speed up the overall modell fitting process dramatically.</p>
</section>
<section id="fit-models" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5"><span class="header-section-number">5.4.5</span> Fit models</h3>
<p>This task involves fitting all combinations of the models. As each new model is fit, the <strong>Model Logs* pane of the </strong>Dashboard** page will be updated with a running progress (model number out of a total), zone/variable/standardisation name along with a message indicating whether the model was run or retrieved from a previous run. Each single model is expected to take approximately 1 minute to run (depending on the clock speed of the computer) so adjust your expectations accordingly.</p>
</section>
<section id="validate-models" class="level3" data-number="5.4.6">
<h3 data-number="5.4.6"><span class="header-section-number">5.4.6</span> Validate models</h3>
<p>This task will perform a range of model validation checks and assign flags against models that display sub-optimal characteristics. Similar to the model fitting task, the status of validation can be tracked in the <strong>Model Log</strong> pane of the <strong>Dashboard</strong> page. Details of the validations performed are given in section <a href="#sec-validation" class="quarto-xref">Section 6.5.2.1</a>.</p>
</section>
<section id="compile-zone-results" class="level3" data-number="5.4.7">
<h3 data-number="5.4.7"><span class="header-section-number">5.4.7</span> Compile zone results</h3>
<p>This task will extract posteriors and summaries for model derived cell means (estimates for each year) for each zone along with effects (comparisons between sets of years). With respect to the effects, the comparisons are:</p>
<ul>
<li>Baseline to each subsequent year</li>
<li>Most recent year to the year prior to that</li>
</ul>
<p>The full posteriors of each of the above are stored in files to be accessed from the <strong>Analysis</strong> page.</p>
</section>
<section id="collect-zone-results" class="level3" data-number="5.4.8">
<h3 data-number="5.4.8"><span class="header-section-number">5.4.8</span> Collect zone results</h3>
<p>This task collects file pointers across all models together into a single file for more convenient access in the <strong>Data</strong> page.</p>
</section>
<section id="compile-site-results" class="level3" data-number="5.4.9">
<h3 data-number="5.4.9"><span class="header-section-number">5.4.9</span> Compile site results</h3>
<p>Similar to the <strong>Compile zone results</strong> this task extracts posteriors and summaries for model derived cell means (estimates for each year) for each site along with effects (comparisons between sets of years). With respect to the effects, the comparisons are:</p>
<ul>
<li>Baseline to each subsequent year</li>
<li>Most recent year to the year prior to that</li>
</ul>
<p>The full posteriors of each of the above are stored in files to be accessed from the <strong>Analysis</strong> page.</p>
</section>
<section id="collect-site-results" class="level3" data-number="5.4.10">
<h3 data-number="5.4.10"><span class="header-section-number">5.4.10</span> Collect site results</h3>
<p>This task collects file pointers across all models together into a single file for more convenient access in the <strong>Data</strong> page.</p>
</section>
<section id="compile-area-results" class="level3" data-number="5.4.11">
<h3 data-number="5.4.11"><span class="header-section-number">5.4.11</span> Compile area results</h3>
<p>This task aggregates the zone level posteriors together before re-calculating the cell means and effects.</p>
</section>
<section id="collect-area-results" class="level3" data-number="5.4.12">
<h3 data-number="5.4.12"><span class="header-section-number">5.4.12</span> Collect area results</h3>
<p>This task collects file pointers across all models together into a single file for more convenient access in the <strong>Data</strong> page.</p>
<p>Finally all site, zone and area results are concatenated together into a single file.</p>
</section>
</section>
</section>
<section id="app-pages" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> App pages</h1>
<section id="sec-landing" class="level2" data-number="6.1">
<h2 data-number="6.1"><span class="header-section-number">6.1</span> Landing page</h2>
<p>To run this tool, please adhere to the following steps:</p>
<ol type="1">
<li>review the <em>Path Settings</em> (specifically checking the <strong>“Data input dir”</strong> and ensuring that there is at least one data file listed in the box under this setting</li>
<li>review the <em>Run Settings</em>. In particular,
<ul>
<li>consider whether you need to <strong>Clear the previous data</strong> - clicking the button to do so. Clearing the previous data deletes all cache and ensure that the analyses are performed fresh. <strong>This is recommended whenever the input data changes</strong>. Not clearing the previous data allows the user to skip directly to later run stages if earlier stages have already been run.</li>
<li>consider the Limit of Reporting (LoR) setting.
<ul>
<li>the default is to set the value equal to the specified limit of reporting for a value (such values must start with a “&lt;”) and will be flagged as “left” censored. Models that accommodate censored data take a probabilistic approach to inferring the likely distribution of all observations including those beyond the limit of reporting and are considered more appropriate.</li>
<li>the alternative is the more traditional approach of replacing the value with 1/2 of the limit of reporting value and using this in the analyses. Whilst traditional, this approach tends to make the resulting values into outliers and thus problematic in analyses.</li>
</ul></li>
</ul></li>
<li>navigate the <em>Dashboard</em> via the menu on the left sidebar</li>
</ol>
</section>
<section id="sec-dashboard" class="level2" data-number="6.2">
<h2 data-number="6.2"><span class="header-section-number">6.2</span> Dashboard</h2>
<p>The analysis pipeline comprises numerous <strong>Stages</strong>, each of which is made up of several more specific <strong>Tasks</strong>. The individual Tasks represent an action performed in furtherance of the analysis and of which there are reportable diagnostics. For example, once the application loads, the first Stage of the pipeline is to prepare the environment. The first Task in this Stage is to load the necessary R packages used by the codebase. Whilst technically, this action consists of numerous R calls (one for each package that needs to be loaded), the block of actions are evaluated as a set.</p>
<p>Initially, all upcoming tasks are reported as “pending” (<span class="fas fa-clock"></span>). As the pipeline progresses, each Task is evaluated and a status is returned as either “success” (<span class="fas fa-circle-check"></span>) or “failure” (<span class="fas fa-circle-xmark"></span>).</p>
<p>The Stage that is currently (or most recently) being run will be expanded, whereas all other Stages will be collapsed (unless they contain errors). It is also possible to expand/collapse a Stage by double clicking on its title (or the small arrow symbol at the left side of the tree).</p>
<p>As the pipeline progresses, Task logs are written to a log file and echoed to the <strong>Logs</strong> panel. Each row represents the returned status of a specific Task and are formatted as:</p>
<ul>
<li>the time/date that the Task was evaluated</li>
<li>the Task status, which can be one of:
<ul>
<li><code>SUCCESS</code> the task succeeded</li>
<li><code>FAILURE</code> the task failed and should be investigated</li>
<li><code>WARNING</code> the task contained a warning - typically these can be ignored as they are usually passed on from underlying routines and are more targetted to developers than users.</li>
</ul></li>
<li>the Stage followed by the Task name</li>
<li>in the case of errors and warnings, there will also be the error or warning message passed on from the underlying routines. These can be useful for helping to diagnose the source and cause of issues.</li>
</ul>
<p>The Logs in the Log panel are presented in chronological order and will autoscroll such that the most recent log is at the bottom of the display. If the number of Log lines exceeds 10, a scroll bar will appear on the right side of the panel to help reviewing earlier Logs.</p>
<div class="call-info callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class='callout-icon no-icon'></i>
</div>
<div class="callout-body-container">
<h4>
Note
</h4>
<p>The Status and Logs are completely refreshed each time the application is restarted.</p>
</div>
</div>
</div>
<p>The Progress panel also has a tab (called <strong>Terminal-like</strong>) which provides an alternative representation of the status and progress of the pipeline.</p>
<p>Under the <strong>Logs</strong> panel, there is a <strong>Model Logs</strong> panel. This panel provides additional status and progress about the fitting and processing of individual statistical models.</p>
</section>
<section id="sec-data" class="level2" data-number="6.3">
<h2 data-number="6.3"><span class="header-section-number">6.3</span> Data</h2>
<p>The Data page comprises two panels or subpages accessable by tabs named “Raw data” and “Processed data” at the top of the page.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The contents of the Processed data subpage will not be revealed until the completion of Stage 3.</p>
</div>
</div>
<section id="raw-data-panel" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1"><span class="header-section-number">6.3.1</span> Raw data panel</h3>
<p>The <strong>Raw data</strong> panel displays the input data and associated validation summaries (once the data have been loaded - that is, once Stage 2 has been complete). The table above initially has a row for each of the input files.</p>
<p>The title of each input file name is displayed in the first column (<strong>File</strong>). The size and file creation time in the next two columns (fields). The <strong>Sheet</strong> field lists the parsed sheets within the excel file and the <strong>Status</strong> column indicates whether all the sheets are valid (<span class="far fa-circle-check"></span>) or not (<span class="far fa-circle-xmark"></span>).</p>
<p>To the left of the file name there is a black triangle. This is an content expansion marker. When the triangle points to the right, clicking anywhere in the cell containing the triangle will expand the table to reveal additional rows (one for each of the sheets in that excel file). The rows can be collapsed again by clicking on the cell containing the downward pointing triangle.</p>
<p>When the additional rows are visible, the <strong>Status</strong> field icons indicate whether the sheet was valid (<span class="fas fa-circle-check"></span>) or not (<span class="fas fa-circle-xmark"></span>).</p>
<p>Clicking on the cell containing the name for a Sheet will make this the <em>focal</em> Sheet of the <strong>Data</strong> and <strong>Validation</strong> tabs:</p>
<ul>
<li><p>the table in the <strong>Data</strong> tab displays the <em>focal</em> content of the input data Sheet. Only the first 10 rows are displayed in the table, the others being accessable via the controls under the table.</p>
<p>Note, all numerical values are displayed only to three decimal places, yet the actual underlying data is full resolution.</p></li>
<li><p>the table in the <strong>Validation</strong> tab displays more details about which fields or rows of the <em>focal</em> Sheet failed validation tests.</p>
<p>If there were no validation issues, this table will be empty. Otherwise, the description field will indicate the nature of the violation and in the case of issues with an individual record, the offending row will be presented across the remaining cells in the row. For more information about the validation tests, please refer to the <strong>Data requirements</strong> box (to the right of this box in the app).</p></li>
</ul>
<p>Underneath both the Data and Validation tables, there is a <strong>Download as csv</strong> button. Via this button, you can download a comma separated text file version of the data in the table for further review in a spreadsheet of your choice. Once you click this button, you will be prompted to navigate to a suitable location to store the file.</p>
</section>
<section id="processed-data-panel" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2"><span class="header-section-number">6.3.2</span> Processed data panel</h3>
<p>The Processed data panel displays the first 10 rows of the complete, compiled and processed data set. Descriptions of each field of these data are provided in the table below.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This panel will not become active until the completion of Stage 3.</p>
</div>
</div>
<p>The <strong>Processed data</strong> panel displays the processed data. As part of the processing, the following new fields will be created:</p>
<div class="table-minimal">
<table class="caption-top">
<colgroup>
<col style="width: 20%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample_key</td>
<td>this is a unique ID for each sample and is created as the combination of <code>Sample_ID</code>, <code>Acquired_date_time</code> and any replicate/duplicate tokens in the <code>Sample_ID</code></td>
</tr>
<tr class="even">
<td>*¹ (mg/kg)</td>
<td>observed concentration of metal in sediment sample</td>
</tr>
<tr class="odd">
<td>Type</td>
<td>whether the record pertains to a metal or hydrocarbon</td>
</tr>
<tr class="even">
<td>Year_cal</td>
<td>calendar year of sample collection</td>
</tr>
<tr class="odd">
<td>Year_fiscal</td>
<td>financial year of sample collection</td>
</tr>
<tr class="even">
<td>Year_water</td>
<td>water year (1st Oct - 30 Sept) of sample collection</td>
</tr>
<tr class="odd">
<td>Year</td>
<td>calendar year of sample collection</td>
</tr>
<tr class="even">
<td>Baseline</td>
<td>whether the record is a baseline record (TRUE) or not (FALSE)</td>
</tr>
<tr class="odd">
<td>ZoneName</td>
<td>name of the Darwin Harbour Zone</td>
</tr>
<tr class="even">
<td>Region</td>
<td>Darwin Harbour Region number</td>
</tr>
<tr class="odd">
<td>RegionName</td>
<td>name of the Darwin Harbour Region</td>
</tr>
<tr class="even">
<td>Zone</td>
<td>Darwin Harbour Zone number</td>
</tr>
<tr class="odd">
<td>Area</td>
<td>Darwin Harbour Area (Inner or Outer)</td>
</tr>
<tr class="even">
<td>Site</td>
<td>ID of the sampling site</td>
</tr>
<tr class="odd">
<td>Var</td>
<td>name of the measurement</td>
</tr>
<tr class="even">
<td>Values</td>
<td>the observed measurement</td>
</tr>
<tr class="odd">
<td>Value_type</td>
<td>whether the value is a standardised value or not</td>
</tr>
<tr class="even">
<td>Fe/Al</td>
<td>Fe:Al where appropriate</td>
</tr>
<tr class="odd">
<td>Fe_Al_normalisation</td>
<td>what the sample <strong>would be</strong> normalised against Fe or Al (or not applicable) if it could be normalised</td>
</tr>
<tr class="even">
<td>Normalised against</td>
<td>what the sample <strong>was</strong> normalised against (if it was normalised)</td>
</tr>
<tr class="odd">
<td>Normalisation flag</td>
<td>whether the normalisation has switched between Fe and Al over time for this site</td>
</tr>
</tbody>
</table>
</div>
<p>Under the column (field) headings in the Processed data panel table, there are input boxes that act as filters. The data presented in the table will be refined to just those cases that match the input string as it is being typed. It is possible to engage with any or all of these filters to help refine the search.</p>
<p>Under the table there is a <strong>Download as csv</strong> button. Via this button, you can download a comma separated text file version of the data in the table for further review in a spreadsheet of your choice. Once you click this button, you will be prompted to navigate to a suitable location to store the file.</p>
</section>
</section>
<section id="exploratory-data-analysis" class="level2" data-number="6.4">
<h2 data-number="6.4"><span class="header-section-number">6.4</span> Exploratory Data Analysis</h2>
<p>The Exploratory Data Analysis page comprises four panels or subpages accessable by tabs at the top of the page and named “Temporal”, “Temporal Type”, “Temporal Site” and “Spatial Type”.</p>
<section id="temporal" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1"><span class="header-section-number">6.4.1</span> Temporal</h3>
<p>This page displays a multi-panel figure depicting the temporal sampling design of the data within each of the Harbour Zones. The y-axis (rows) of the figure represents the Sampling sites (based on the Site names they were first assigned). Blue points represent the samples collected that are considered to be “Baseline” or “Reference” samples from which subsequent samples at the corresponding site are to be gauged (compared). Red points represent non-“Baseline” samples. Points are jointed by lines to help identify discontinued sampling (where no line exists) and where no Baselines have been defined (when the left point of a sequence is red).</p>
<p>To keep the figure to a manageable size and degree of clutter, the figure fucusses only on “Sites” that were first monitored in the semester indicated by the dark blue selector to the left side of the figure. The selectors are partitioned into 6 monthly (a semester) chunks and clicking on a different selector will update the figure to a different fraction of the data.</p>
</section>
<section id="temporal-type" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2"><span class="header-section-number">6.4.2</span> Temporal Type</h3>
<p>The multi-panel figure displayed on this page partitions the data into Zone/Measurement types (via left hand selectors) and figure columns (measures). The x-axis (rows) of the figure represents the sampling years and the y-axis represents the individual sampling sites. Blue points represent the samples collected that are considered to be “Baseline” or “Reference” samples from which subsequent samples at the corresponding site are to be gauged (compared). Red points represent non-“Baseline” samples.</p>
</section>
<section id="temporal-site" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3"><span class="header-section-number">6.4.3</span> Temporal Site</h3>
<p>This page provides a way of fully exploring the un-modelled temporal trends in both raw and standardised data for each Site, Measurement Type and Variable.</p>
<p>Blue points represent the samples collected that are considered “Baseline” or “Reference” samples from which subsequent samples at the corresponding site are gauged. Red points represent non-“Baseline” samples. Multiple panels (if present) distinguish standardised and unstandardised data. Hovering over the points will reveal some of the pertinent underlying data associated with the point.</p>
<p>Different combinations of Sites, Measurement Type and Variables can be selected via their respective dropdown boxed in the panel above the figures.</p>
</section>
<section id="spatial-type" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4"><span class="header-section-number">6.4.4</span> Spatial Type</h3>
<p>This page provides a way to visualise the spatial distribution of un-modelled data on a map. It also provides a representation of the temporal trends in the data. The subset of data presented in the temporal graph and on the map are controlled by a set of dropdown selector boxes. The data presented on the map are also further refined to a single year and that is controlled by the slider situated above the temporal figure.</p>
<p>In the temporal figure, each point represents an individual observation and observations collected from the same sites over time are connected by gray lines. Blue points represent the samples collected that are considered “Baseline” or “Reference” samples from which subsequent samples at the corresponding site are gauged. Red points represent non-“Baseline” samples. Multiple panels (if present) distinguish standardised and unstandardised data. Hovering over the points will reveal some of the pertinent underlying data associated with the point.</p>
<p>Sites are represented on the map by circular points, the colour of which are mapped to a scale proportional to the data values. Panning and zooming of the map is done via the mouse (moving while left mouse press: panning, scrolling of mouse button: zooming). The Zone selector dropdown controls which Harbour Zone(s) are highlighted and populated with data.</p>
</section>
</section>
<section id="analysis" class="level2" data-number="6.5">
<h2 data-number="6.5"><span class="header-section-number">6.5</span> Analysis</h2>
<p>The Analysis page comprises three panels or subpages accessable by tabs at the top of the page and named “Analysis overview”, “Model diagnostics” and “Analysis details”.</p>
<section id="analysis-overview" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1"><span class="header-section-number">6.5.1</span> Analysis overview</h3>
<p>The main feature of this panel is a table representing a very high level overview of the results conditional on spatial scale (Site, Zone, Area), Measurement Type (hydrocarbons or metals), Value Type (Unstandardised or Standardised), Normalisation (Standardisation) method, Focal Year and Contrast (each of which is controlled via a dropdown).</p>
<p>The table presents a matrix in which:</p>
<ul>
<li><p>the columns represent the variables</p></li>
<li><p>the rows represent the sites/zones/areas</p></li>
<li><p>the colour of the cells represents the polarity and evidence of change:</p>
<ul>
<li>red: strong evidence of an increase</li>
<li>orange: evidence of an increase</li>
<li>yellow: weak evidence of an increase</li>
<li>white: no evidence of change</li>
<li>light green: weak evidence of a decline</li>
<li>mid green: evidence of a decline</li>
<li>dark green: strong evidence of a decline</li>
<li>gray: model not completed (typically due to a lack of data)</li>
</ul></li>
<li><p>models built on data that includes values below limit of reporting/detection are marked with a black flag (<span class="far fa-flag"></span>)</p></li>
</ul>
<p>The selectors can be used to filter what subset of data are presented in the table:</p>
<ul>
<li>scale of aggregation (site/zone/area)</li>
<li>whether or not the data are standardised</li>
<li>the type of variable (metals, hydrocarbons or both)</li>
<li>the focal year to compare to the baseline (2019/2020)</li>
</ul>
<p>At the top of this page there is a collapsed box containing an overview of the model validation checks. To view the table, ensure that the box is expanded (click on the “-” icon in the upper right corner of the box banner).</p>
<p>The table is filterable (by entering text in the text boxes between the column headings and first row of data). Each row represents the inforation about a single model. The validity tests represent whether there is any evidence to reject (p-value &lt; 0.05). A value of “fail” indicates that there is evidence that the test revealed evidence to invalidate the assumption:</p>
<ul>
<li><strong>ks</strong> the KS (uniformity test)</li>
<li><strong>ds.p</strong> the dispersion test</li>
<li><strong>q</strong> residual plot quantiles</li>
<li><strong>o</strong> outlier observations</li>
<li><strong>valid</strong> whether any of the above failed</li>
</ul>
<p>The table is initially sorted such models flagged as potentially invalid are towards the top.</p>
<p>The page also displays diagnostics and summaries of each of the fitted models accessible via hierarchical tabs and selectable via Zone, Variable and Standardisation selection boxes.</p>
<p>More information about each of the validation and summarisations are provided inside the corresponding set of tabs.</p>
</section>
<section id="model-diagnostics" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2"><span class="header-section-number">6.5.2</span> Model diagnostics</h3>
<p>This panel displays a wide range of MCMC sampling and model validation diagnostics as well as simple model summaries.</p>
<section id="sec-validation" class="level4">
<h4>Model Validation</h4>
<section id="prior-vs-posterior" class="level5">
<h5>Prior vs Posterior</h5>
<p>Plots of priors (black points and whiskers) and posteriors (coloured points and whiskers) for each of the main model parameters. This sort of figure provides a way to visualy assess how <strong>informative</strong> the priors are likely to be in determining the posteriors. Ideally, we want the priors to only be <strong>weakly</strong> informative. That is, we only want them to have a regularising influence (encouraging the sampler to stay within a range of parameter estimates that are vaguely plausible). Hence, the ideal is for the priors (black) to be either substantially wider (longer in the figure) or centered at a different location (position along y-axis) than the corresponding posteriors (coloured).</p>
</section>
<section id="traceplots" class="level5">
<h5>Traceplots</h5>
<p>MCMC traceplots are visual tools used to assess the convergence and mixing behavior of Markov Chain Monte Carlo (MCMC) algorithms in Bayesian statistics. They offer valuable insights into the efficiency and reliability of your MCMC analysis.</p>
<p><strong>What do they show?</strong></p>
<p>Each trace plot displays the values of a specific parameter estimated by the MCMC algorithm across all iterations. Multiple parameters can be plotted individually or jointly to compare their behavior.</p>
<p><strong>How to interpret them:</strong></p>
<ul>
<li><p><strong>Convergence:</strong></p>
<p>A well-converged chain should exhibit stability over iterations, meaning the trace plot plateaus around a constant value or fluctuates within a predictable range. Trends or drifts indicate the chain hasn’t converged, and the estimates might be biased.</p></li>
<li><p><strong>Mixing:</strong></p>
<p>Good mixing implies the chain explores the parameter space efficiently, visiting different possible values frequently. Stuck chains remain in specific regions, leading to poor exploration and unreliable estimates.</p></li>
<li><p><strong>Autocorrelation:</strong></p>
<p>If consecutive values in the trace plot are highly correlated, the chain mixes slowly, impacting efficiency.</p></li>
<li><p><strong>Mixing across chains:</strong></p>
<p>Running multiple chains (starting from different points) should converge to similar values, supporting mixing and reliability.</p></li>
</ul>
<p><strong>Interpretation tips:</strong></p>
<ul>
<li>Consider the expected behavior of your model: some parameters might naturally fluctuate more than others.</li>
<li>Use reference values or theoretical limits to judge if the parameter values seem reasonable.</li>
<li>Combine trace plots with other diagnostics like Gelman-Rubin convergence measures for a comprehensive assessment.</li>
</ul>
</section>
<section id="autocorrelation-plots" class="level5">
<h5>Autocorrelation plots</h5>
<p>MCMC autocorrelation plots, are another essential tool for diagnosing the convergence and mixing of your MCMC algorithm in Bayesian statistics.</p>
<p><strong>What are they?</strong></p>
<p>Autocorrelation plots measure the correlation between values of a parameter at different lags (distances) within the MCMC chain. They typically display the correlation coefficient at each lag, plotted against the lag number. The first bar represents the correlation of MCMC samples with themselves, and thus will always be 1 on the y-axis.</p>
<p><strong>How to interpret them:</strong></p>
<ul>
<li><p><strong>Bias:</strong></p>
<p>The autocorrelation plot helps assess how quickly the correlation between samples decays as the lag increases. Ideally, samples collected from any process (including MCMC sampling) should all be independent and unbiased in order to provide unbiased estimates. If the autocorrelation values decay rapidly to zero as the lag increases, it indicates that the MCMC samples are likely to be independent and unbiased. Conversely, if high autocorrelation persists with large lags, it is possible that the MCMC samples are biased towards particular regions of the parameter space. **Ideally, the degree of autocorrelation should drop to below 0.25 by the second bar (second lag).</p></li>
<li><p><strong>Convergence:</strong></p>
<ul>
<li><p>Rapidly decaying autocorrelation: As the lag increases, the correlation between values drops quickly, indicating good convergence. The chain efficiently moves through the parameter space, providing reliable and unbiased estimates.</p></li>
<li><p>High autocorrelation even at large lags: This suggests the chain is “stuck” in certain regions, not exploring the parameter space well. Estimates might be biased and unreliable.</p></li>
</ul></li>
<li><p><strong>Mixing:</strong></p>
<ul>
<li><p>High autocorrelation at small lags: The chain takes many iterations to “forget” past values, indicating slow mixing. The algorithm might not be efficiently exploring the parameter space.</p></li>
<li><p>Autocorrelation dropping to zero around medium lags: This suggests the chain mixes reasonably well, considering the natural dependence between consecutive values.</p></li>
</ul></li>
</ul>
<p>If autocorrelation patterns exist in the MCMC samples, the MCMC samples should be thinned to a higher degree.</p>
</section>
<section id="rhat-plots" class="level5">
<h5>Rhat plots</h5>
<p>Rhat plots, also known as Gelman-Rubin convergence diagnostic plots, are graphical tools used to assess the convergence of multiple chains in a Bayesian analysis. These plots are based on the Gelman-Rubin statistic (Rhat), which compares the variance within chains to the variance between chains.</p>
<p><strong>What are they?</strong></p>
<p>Rhat represents a “potential scale reduction factor”, and compares the within-chain and between-chain variances of parameter estimates from multiple MCMC chains. It’s used to assess whether the chains have mixed well and converged to the same target distribution.</p>
<p>Rhat plots visualise the Rhat statistic for each parameter estimated in the Bayesian analysis. Each parameter has its own Rhat value, and the plot typically displayed as histograms.</p>
<p><strong>How to interpret them:</strong></p>
<ul>
<li><p><strong>Rhat &lt; 1.01:</strong></p>
<p>Generally indicates very good convergence, suggesting the chains have explored the target distribution efficiently and reached similar conclusions.</p></li>
<li><p><strong>Rhat &lt; 1.05:</strong></p>
<p>Generally indicates acceptable convergence.</p></li>
<li><p><strong>Rhat &gt; 1.05:</strong></p>
<p>Indicates significant convergence problems, meaning the chains haven’t mixed well and could lead to biased estimates. It implies that at least one of the chains may have traversed different features of the parameter space compared to the other chain(s). If so, then there may well be other additional un-traversed features. It is possible that the MCMC sampler may not have fully explored all important regions of the parameter space.</p></li>
</ul>
</section>
<section id="effective-sample-size" class="level5">
<h5>Effective Sample Size</h5>
<p>ESS plots, or Effective Sample Size plots, are graphical tools used to assess the effective sample size of MCMC samples obtained from Bayesian analyses. The effective sample size quantifies the amount of independent information contained in the MCMC samples and is crucial for accurate estimation of posterior quantities.</p>
<p><strong>How do they work?</strong></p>
<p>Imagine your MCMC chain has length <span class="math inline">\(n\)</span>, but due to autocorrelation, not all <span class="math inline">\(n\)</span> iterations provide truly independent information. ESS attempts to quantify this by estimating the number of independent samples equivalent to your chain, effectively capturing the information content.</p>
<p><strong>How to interpret them:</strong></p>
<ul>
<li><p><strong>High ESS (closer to <span class="math inline">\(n\)</span>):</strong></p>
<p>This indicates good efficiency, meaning your chain efficiently explores the parameter space and provides reliable estimates.</p></li>
<li><p><strong>Low ESS (much smaller than <span class="math inline">\(n\)</span>):</strong></p>
<p>Suggests poor efficiency, with high autocorrelation reducing the effective number of independent samples. This can lead to wider credible intervals and less precise estimates.</p></li>
</ul>
<p>In order to have enough MCMC samples to provide meaningful summaries, each parameter should have at least 1000 effective samples.</p>
<p>The ESS values are often expressed as fractions of the total <span class="math inline">\(n\)</span> and represented graphically as a histogram. Ideally, most (if not all) the ESS values should be above 0.5.</p>
<p>If there are low ESS, it suggests that either the total number of iterations was not large enough or the sampler was not sampling efficiently. For the former case, the sampler should be re-run with additional iterations. In the later case, it is likely that the model itself (or the priors) are mis-specified and this should be addressed before rerunning the model.</p>
</section>
<section id="posterior-probability-plots" class="level5">
<h5>Posterior Probability plots</h5>
<p>Posterior probability density overlay plots display the density distribution of the observed data (dark line) overlayed upon the density distributions of a large number of posterior predictions (fainter blue lines).</p>
<p>The general idea is that if a model is to be useful, it should be able to generate observations (predictions) that approximate the data used to train the model in the first place. If this is not the case, then it is likely that the model does not adequately capture the pertinent properties of the observed data.</p>
<p>Ideally, the densities of the numerous realisations from the posteriors should match closely the density of the original observed data. The manner by which the densities differ from that of the original data can be used to infer what aspects of the data the model is under or over estimating and adjustments can be made to the model accordingly.</p>
</section>
<section id="simulated-dharma-residuals" class="level5">
<h5>Simulated (DHARMa) residuals</h5>
<p>Since statistical models are low dimensional representations of a system the reliability of a statistical model will depend on the degree to which certain assumptions are met. Many of these assumptions can be explored by examination of the model residuals.</p>
<p>Patterns in residuals suggest either issues of dependencies (biases), poor model structure and a general lack of model fit. However, for many statistical models, discerning genuinely violation-indicating patterns in residuals from artifacts due to the model type can be really difficult - if not impossible.</p>
<p>Within R, the <em>DHARMa</em> (Diagnostics for HierArchical Regression Models: <span class="citation" data-cites="DHARMa">Hartig (<a href="#ref-DHARMa" role="doc-biblioref">2022</a>)</span>) package generates standardised residuals via simulation and uses these as the basis of a range of tools to diagnose common modelling issues including outliers, heterogeneity, over-dispersion, autocorrelation.</p>
<p>New observations simulated from the fitted model are used to calculate a cumulative density function (CDF) that describes the probability profile of each observation. Thereafter, the residual of an observation is calculated as the value of the CDF that corresponds to the actual observed value:</p>
<ul>
<li>a value of 0 indicates that the observed value was less than all simulated values</li>
<li>a value of 1 indicates that the observed value was greater than all simulated values</li>
<li>a value of 0.5 indicates that the probability of obtaining the observed value is 50%.</li>
</ul>
<p>This approach ensures that all residuals have the same interpretation irrespective of the model and distribution selected.</p>
<p><em>DHARMa</em> supports a variety of diagnostic plots based on the simulated residuals, including residual plots, QQ plots, and test-based diagnostic plots. These plots allow for visual inspection of model adequacy and can reveal patterns or deviations indicative of model misspecification or violation of assumptions. The most common plots and their interpretations are:</p>
<ul>
<li><p>a <strong>Q-Q plot</strong></p>
<p>Ideally all points should be close to the diagonal red line. Overlayed onto this plot are three additional tests.</p>
<ol type="1">
<li>KS (Kolmogorov-Smirnov) test investigates whether the (in this case simulated) are likely to have been drawn from the nominated distribution.</li>
<li>Dispersion test investigates whether the is any evidence of overdispersion (more variability than the model expects) estimated as the standard deviation of the data is equal to that of the simulated data)</li>
<li>Outlier test investigates for the prevalence of outliers (when observed values are outside the simulated range)</li>
</ol></li>
<li><p>a <strong>Residual vs Predicted plot</strong></p>
<p>Ideally, there should be no patterns in the residuals. To help identify any patterns, quantile trends are overlayed. Ideally, there should be a flat black line at each of the quantiles of 0.25, 0.5 and 0.75. In some circumstances, quantiles cannot be computed and in such cases a single dashed smoother many be placed over the data cloud.</p></li>
<li><p>a <strong>Dispersion plot</strong></p>
<p>The observed model dispersion is overlayed (red line) upon the distribution (black) of simulated dispersion values. Ideally the red line should be in the middle of the simulated distribution.</p>
<ul>
<li>If the red line is to the far right, the model is considered overdispersed. Parameter uncertainty is typically underestimated in overdispersed models - this leads us to be more confident in our results than we should be and this is bad.</li>
<li>If the red line is to the far left, the model is considered underdispersed. This is usually an artifact and results in our estimates being more conservative than they perhaps should be. Underdispersion is less of an issue as it just results in more conservatism in results.</li>
</ul></li>
</ul>
<p><strong>Note</strong>, the individual tests that accompany the diagnostic plots tend to be stricter than the assumptions we are seeking to explore. That is, statistical models tend to be reasonably robust to mild assumption violations, yet the diagnostic tests are fairly strict. Hence, the tests are used to flag potential issues, yet the ownace is still on the researchers to explore these violations in greater detail and evaluate whether they are likely to have any important consequences.</p>
</section>
</section>
<section id="model-summaries" class="level4">
<h4>Model Summaries</h4>
<p>The coefficients (parameter estimates) table displays each of the main model parameter estimates on the scale of the link function. Parameters that start with <code>b_</code> are the population-level (fixed) effects. Parameters that start with <code>sd_</code> are the varying (random) effects.</p>
<p>The table lists the posterior median as well as lower and upper bounds of the 95% Highest Posterior Density (HPD) interval for each parameter. Also tabulated are the total number of posterior draws (<code>length</code>), Rhat values (<code>rhat</code>), and effective sample size in both the bulk of the posterior (<code>ess_bulk</code>) and tail of the posterior (<code>ess_tail</code>).</p>
</section>
</section>
<section id="analysis-details" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3"><span class="header-section-number">6.5.3</span> Analysis details</h3>
<p>This panel has two sub-panels for displaying “Modelled trends” and “Modelled effects”.</p>
<section id="modelled-trends" class="level4">
<h4>Modelled trends</h4>
<p>The modelled (predicted) values of a focal variable are displayed in tabular and graphical form.</p>
<p>In the table, each row represents the summaries of predictions for a specific year. The summaries are in the form of median as well as lower and upper 95% highest probability density (HPD) intervals of the full model posteriors. Years in which data included values below the limit of reporting are marked by a black flag (<span class="far fa-flag"></span>). The table also indicates which years are considered Baseline.</p>
<p>These same summaries are also presented graphically in the figure. Additionally, the figure displays the underlying un-modelled data in gray. Individual sites are connected by gray lines.</p>
</section>
<section id="modelled-effects" class="level4">
<h4>Modelled effects</h4>
<p>This table summarises the comparisons between the average of the baseline year(s) and each of the subsequent reporting years. Values are reported on a <strong>fold</strong> (fractional) scale. On this scale, a value of 1 indicates no change. Values of 0.5 and 2 would indicate that the value in the reporting year is respectively half and twice that of the baseline year(s).</p>
<p>Posteriors of the comparisons are summarised by their medians as well as the lower and upper bounds of the 95% Highest Posterior Density (HPD) interval. Also tabulated are the exceedence probabilities associated with the probability that the values have declined since the baseline year(s) (<code>Pl</code>) and the probabilities that they have increased (<code>Pg</code>). These probabilities must sum to 1 and it is only necessary to explore the larger value per comparison. As a guide exeedence probabilities:</p>
<ul>
<li><strong>&gt;0.95</strong> provide strong evidence of an effect (change)</li>
<li><strong>&gt;0.9</strong> provide evidence of an effect (change)</li>
<li><strong>&gt;0.85</strong> provide week evidence of an effect (change)</li>
</ul>
<p>Finally, there is also a column that represents the polarity and evidence of change via a color.</p>
<ul>
<li>red: strong evidence of an increase</li>
<li>orange: evidence of an increase</li>
<li>yellow: weak evidence of an increase</li>
<li>white: no evidence of change</li>
<li>light green: weak evidence of a decline</li>
<li>mid green: evidence of a decline</li>
<li>dark green: strong evidence of a decline</li>
</ul>
<p>The full modelled posteriors are depicted in the figure under the table. The y-axis represents each of the contrasts (comparisons between sets of years or years and the Baseline). The posterior distributions are coloured according to the polarity and degree of evidence as outlined above.</p>
<p>The full and summarised posteriors can be presented on either a <strong>percentage</strong> scale (default) or <strong>fold</strong> scale and this is controlled via a dropdown box above the table.</p>
<ul>
<li>the percentage scale is interpreted as the percentage change between the contrasting years. For example, percentage changes of -20, 20 and 0 indicate a 20% decline, 20% increase and no change respectively.</li>
<li>the fold scale is interpreted as the fold change between the contrasting years. For example, fold changes of 0.5, 2 and 1 indicate a halving, a doubling and no change respectively.</li>
</ul>
<p><i class="fa-solid fa-thumbs-up" aria-label="thumbs-up"></i></p>
<!-- -->
<div class="quarto-embedded-source-code">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="an">title:</span><span class="co"> &quot;Darwin Harbour sediment monitoring program analysis application manual&quot;</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="an">author:</span><span class="co"> &quot;Murray Logan&quot;</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="an">date-format:</span><span class="co"> &quot;DD/MM/YYYY&quot;</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">  html:</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">    ## Format</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co">    theme: spacelab</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co">    css: resources/style.css</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co">    html-math-method: mathjax</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">    ## Table of contents</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co">    toc: true</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co">    toc-float: true</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">    ## Numbering</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="co">    number-sections: true</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="co">    number-depth: 3</span></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="co">    ## Layout</span></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="co">    fig-caption-location: &quot;bottom&quot;</span></span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="co">    fig-align: &quot;center&quot;</span></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="co">    fig-width: 8</span></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="co">    fig-height: 8</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co">    fig-dpi: 72</span></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="co">    tbl-cap-location: top</span></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="co">    ## Code</span></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co">    code-fold: false</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="co">    code-tools: true</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="co">    code-summary: &quot;Show the code&quot;</span></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">    code-line-numbers: true</span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="co">    code-block-border-left: &quot;#ccc&quot;</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co">    highlight-style: zenburn</span></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="co">    ## Execution</span></span>
<span id="cb2-33"><a href="#cb2-33"></a><span class="co">    execute:</span></span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="co">      echo: true</span></span>
<span id="cb2-35"><a href="#cb2-35"></a><span class="co">    ## Rendering</span></span>
<span id="cb2-36"><a href="#cb2-36"></a><span class="co">    embed-resources: true</span></span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="co">    fontsize: 12pt</span></span>
<span id="cb2-38"><a href="#cb2-38"></a><span class="co">  pdf:</span></span>
<span id="cb2-39"><a href="#cb2-39"></a><span class="co">    toc: true</span></span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="co">    margin-left: 2cm</span></span>
<span id="cb2-41"><a href="#cb2-41"></a><span class="co">    margin-right: 2cm</span></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="co">    margin-top: 2cm</span></span>
<span id="cb2-43"><a href="#cb2-43"></a><span class="co">    margin-bottom: 2cm</span></span>
<span id="cb2-44"><a href="#cb2-44"></a><span class="co">    fig-width: 9</span></span>
<span id="cb2-45"><a href="#cb2-45"></a><span class="co">    fontsize: 8pt</span></span>
<span id="cb2-46"><a href="#cb2-46"></a><span class="co">    keep-tex: true</span></span>
<span id="cb2-47"><a href="#cb2-47"></a><span class="an">crossref:</span></span>
<span id="cb2-48"><a href="#cb2-48"></a><span class="co">  fig-title: &#39;**Figure**&#39;</span></span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="co">  fig-labels: arabic</span></span>
<span id="cb2-50"><a href="#cb2-50"></a><span class="co">  tbl-title: &#39;**Table**&#39;</span></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="co">  tbl-labels: arabic</span></span>
<span id="cb2-52"><a href="#cb2-52"></a><span class="an">engine:</span><span class="co"> knitr</span></span>
<span id="cb2-53"><a href="#cb2-53"></a><span class="an">output_dir:</span><span class="co"> &quot;docs&quot;</span></span>
<span id="cb2-54"><a href="#cb2-54"></a><span class="an">documentclass:</span><span class="co"> article</span></span>
<span id="cb2-55"><a href="#cb2-55"></a><span class="an">mainfont:</span><span class="co"> Arial</span></span>
<span id="cb2-56"><a href="#cb2-56"></a><span class="an">mathfont:</span><span class="co"> LiberationMono</span></span>
<span id="cb2-57"><a href="#cb2-57"></a><span class="an">monofont:</span><span class="co"> DejaVu Sans Mono</span></span>
<span id="cb2-58"><a href="#cb2-58"></a><span class="an">classoption:</span><span class="co"> a4paper</span></span>
<span id="cb2-59"><a href="#cb2-59"></a><span class="an">bibliography:</span><span class="co"> resources/references.bib</span></span>
<span id="cb2-60"><a href="#cb2-60"></a><span class="co">---</span></span>
<span id="cb2-61"><a href="#cb2-61"></a></span>
<span id="cb2-62"><a href="#cb2-62"></a><span class="in">```{r setup, include=FALSE}</span></span>
<span id="cb2-63"><a href="#cb2-63"></a><span class="in">knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)</span></span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="in">options(tinytex.engine = &quot;xelatex&quot;)</span></span>
<span id="cb2-65"><a href="#cb2-65"></a><span class="in">```</span></span>
<span id="cb2-66"><a href="#cb2-66"></a></span>
<span id="cb2-67"><a href="#cb2-67"></a></span>
<span id="cb2-68"><a href="#cb2-68"></a><span class="fu"># About</span></span>
<span id="cb2-69"><a href="#cb2-69"></a></span>
<span id="cb2-70"><a href="#cb2-70"></a>This document comprises the manual for the Darwin Harbour sediment</span>
<span id="cb2-71"><a href="#cb2-71"></a>monitoring program analysis application.  It provides information on:</span>
<span id="cb2-72"><a href="#cb2-72"></a></span>
<span id="cb2-73"><a href="#cb2-73"></a><span class="ss">- </span>a broad overview of the structure of the application</span>
<span id="cb2-74"><a href="#cb2-74"></a><span class="ss">- </span>the application dependencies and how to install them</span>
<span id="cb2-75"><a href="#cb2-75"></a><span class="ss">- </span>starting the application</span>
<span id="cb2-76"><a href="#cb2-76"></a><span class="ss">- </span>progressing through the analysis pipeline</span>
<span id="cb2-77"><a href="#cb2-77"></a><span class="ss">- </span>visualising, interpreting and extracting outputs</span>
<span id="cb2-78"><a href="#cb2-78"></a> </span>
<span id="cb2-79"><a href="#cb2-79"></a><span class="fu"># Structural overview</span></span>
<span id="cb2-80"><a href="#cb2-80"></a></span>
<span id="cb2-81"><a href="#cb2-81"></a><span class="co">[</span><span class="ot">R Graphical and Statistical Environment</span><span class="co">](https://www.r-project.org/)</span></span>
<span id="cb2-82"><a href="#cb2-82"></a>offers an ideal platform for developing and running complex</span>
<span id="cb2-83"><a href="#cb2-83"></a>statistical analyses as well as presenting the outcomes via</span>
<span id="cb2-84"><a href="#cb2-84"></a>professional graphical/tabular representations. As a completely</span>
<span id="cb2-85"><a href="#cb2-85"></a>scripted language it also offers the potential for both full</span>
<span id="cb2-86"><a href="#cb2-86"></a>transparency and reproducibility. Nevertheless, as the language, and</span>
<span id="cb2-87"><a href="#cb2-87"></a>more specifically the extension packages are community developed and</span>
<span id="cb2-88"><a href="#cb2-88"></a>maintained, the environment evolves over time. Similarly, the</span>
<span id="cb2-89"><a href="#cb2-89"></a>underlying operating systems and programs on which R and its extension</span>
<span id="cb2-90"><a href="#cb2-90"></a>packages depend (hereafter referred to as the _operating environment_)</span>
<span id="cb2-91"><a href="#cb2-91"></a>also change over time. Consequently, the stability and reproducibility</span>
<span id="cb2-92"><a href="#cb2-92"></a>of R codes have a tendency to change over time.</span>
<span id="cb2-93"><a href="#cb2-93"></a></span>
<span id="cb2-94"><a href="#cb2-94"></a><span class="fu">## Docker containers</span></span>
<span id="cb2-95"><a href="#cb2-95"></a></span>
<span id="cb2-96"><a href="#cb2-96"></a>One way to attempt to future proof a codebase that must be run upon a</span>
<span id="cb2-97"><a href="#cb2-97"></a>potentially unpredictable operating environment is to **containerise**</span>
<span id="cb2-98"><a href="#cb2-98"></a>the operating environment, such that it is preserved to remain</span>
<span id="cb2-99"><a href="#cb2-99"></a>unchanged over time. Containers (specifically</span>
<span id="cb2-100"><a href="#cb2-100"></a><span class="co">[</span><span class="ot">docker</span><span class="co">](https://www.docker.com/)</span> containers) are lightweight</span>
<span id="cb2-101"><a href="#cb2-101"></a>abstraction units that encapsulate applications and their dependencies</span>
<span id="cb2-102"><a href="#cb2-102"></a>within standardized, self-contained execution environments. Leveraging</span>
<span id="cb2-103"><a href="#cb2-103"></a>containerization technology, they package application code, runtime,</span>
<span id="cb2-104"><a href="#cb2-104"></a>libraries, and system tools into isolated units (_containers_) that</span>
<span id="cb2-105"><a href="#cb2-105"></a>abstract away underlying infrastructure differences, enabling</span>
<span id="cb2-106"><a href="#cb2-106"></a>consistent and predictable execution across diverse computing</span>
<span id="cb2-107"><a href="#cb2-107"></a>platforms.</span>
<span id="cb2-108"><a href="#cb2-108"></a></span>
<span id="cb2-109"><a href="#cb2-109"></a>Containers offer several advantages, such as efficient resource</span>
<span id="cb2-110"><a href="#cb2-110"></a>utilization, rapid deployment, and scalability. They enable developers</span>
<span id="cb2-111"><a href="#cb2-111"></a>to build, test, and deploy applications with greater speed and</span>
<span id="cb2-112"><a href="#cb2-112"></a>flexibility. Docker containers have become a fundamental building</span>
<span id="cb2-113"><a href="#cb2-113"></a>block in modern software development, enabling the development and</span>
<span id="cb2-114"><a href="#cb2-114"></a>deployment of applications in a consistent and predictable manner</span>
<span id="cb2-115"><a href="#cb2-115"></a>across various environments.</span>
<span id="cb2-116"><a href="#cb2-116"></a></span>
<span id="cb2-117"><a href="#cb2-117"></a><span class="fu">## Shiny applications</span></span>
<span id="cb2-118"><a href="#cb2-118"></a></span>
<span id="cb2-119"><a href="#cb2-119"></a><span class="co">[</span><span class="ot">Shiny</span><span class="co">](https://shiny.posit.co/)</span> is a web application framework for R</span>
<span id="cb2-120"><a href="#cb2-120"></a>that enables the creation of interactive and data-driven web</span>
<span id="cb2-121"><a href="#cb2-121"></a>applications directly from R scripts. Developed by</span>
<span id="cb2-122"><a href="#cb2-122"></a><span class="co">[</span><span class="ot">Rstudio</span><span class="co">](https://posit.co/)</span>, Shiny simplifies the process of turning</span>
<span id="cb2-123"><a href="#cb2-123"></a>analyses into interactive web-based tools without the need for</span>
<span id="cb2-124"><a href="#cb2-124"></a>extensive web development expertise.</span>
<span id="cb2-125"><a href="#cb2-125"></a></span>
<span id="cb2-126"><a href="#cb2-126"></a>What makes Shiny particularly valuable is its seamless integration</span>
<span id="cb2-127"><a href="#cb2-127"></a>with R, allowing statisticians and data scientists to build and deploy</span>
<span id="cb2-128"><a href="#cb2-128"></a>bespoke statistical applications, thereby making data visualization,</span>
<span id="cb2-129"><a href="#cb2-129"></a>exploration, and analysis accessible to a broader audience. With its</span>
<span id="cb2-130"><a href="#cb2-130"></a>interactive and user-friendly nature, Shiny serves as a powerful tool</span>
<span id="cb2-131"><a href="#cb2-131"></a>for sharing insights and engaging stakeholders in a more intuitive and</span>
<span id="cb2-132"><a href="#cb2-132"></a>visual manner.</span>
<span id="cb2-133"><a href="#cb2-133"></a></span>
<span id="cb2-134"><a href="#cb2-134"></a><span class="fu">## Git and github</span></span>
<span id="cb2-135"><a href="#cb2-135"></a></span>
<span id="cb2-136"><a href="#cb2-136"></a>Git, a distributed version control system, and</span>
<span id="cb2-137"><a href="#cb2-137"></a><span class="co">[</span><span class="ot">GitHub</span><span class="co">](https://github.com/)</span>, a web-based platform for hosting and</span>
<span id="cb2-138"><a href="#cb2-138"></a>collaborating on Git repositories, play pivotal roles in enhancing</span>
<span id="cb2-139"><a href="#cb2-139"></a>reproducibility and transparency in software development. By tracking</span>
<span id="cb2-140"><a href="#cb2-140"></a>changes in source code and providing a centralized platform for</span>
<span id="cb2-141"><a href="#cb2-141"></a>collaborative work, Git and GitHub enable developers to maintain a</span>
<span id="cb2-142"><a href="#cb2-142"></a>detailed history of code alterations. This history serves as a</span>
<span id="cb2-143"><a href="#cb2-143"></a>valuable asset for ensuring the reproducibility of software projects,</span>
<span id="cb2-144"><a href="#cb2-144"></a>allowing users to trace and replicate specific versions of the</span>
<span id="cb2-145"><a href="#cb2-145"></a>codebase.</span>
<span id="cb2-146"><a href="#cb2-146"></a></span>
<span id="cb2-147"><a href="#cb2-147"></a>GitHub Actions (an integrated workflow automation feature of GitHub),</span>
<span id="cb2-148"><a href="#cb2-148"></a>automates tasks such as building, testing, and deploying applications</span>
<span id="cb2-149"><a href="#cb2-149"></a>and artifacts. Notably, through workflow actions, GitHub Actions can</span>
<span id="cb2-150"><a href="#cb2-150"></a>build docker containers and act as a container registry. This</span>
<span id="cb2-151"><a href="#cb2-151"></a>integration enhances the overall transparency of software development</span>
<span id="cb2-152"><a href="#cb2-152"></a>workflows, making it easier to share, understand, and reproduce</span>
<span id="cb2-153"><a href="#cb2-153"></a>projects collaboratively.</span>
<span id="cb2-154"><a href="#cb2-154"></a></span>
<span id="cb2-155"><a href="#cb2-155"></a>@fig-diagram provides a schematic overview of the relationship</span>
<span id="cb2-156"><a href="#cb2-156"></a>between the code produced by the developer, the Github cloud</span>
<span id="cb2-157"><a href="#cb2-157"></a>repositiory and container registry and the shiny docker container run</span>
<span id="cb2-158"><a href="#cb2-158"></a>by user.</span>
<span id="cb2-159"><a href="#cb2-159"></a></span>
<span id="cb2-160"><a href="#cb2-160"></a></span>
<span id="cb2-161"><a href="#cb2-161"></a></span>
<span id="cb2-162"><a href="#cb2-162"></a>quarto-executable-code-5450563D</span>
<span id="cb2-163"><a href="#cb2-163"></a></span>
<span id="cb2-164"><a href="#cb2-164"></a><span class="in">```r</span></span>
<span id="cb2-165"><a href="#cb2-165"></a><span class="co">#| label: fig-diagram</span></span>
<span id="cb2-166"><a href="#cb2-166"></a><span class="co">#| engine: tikz</span></span>
<span id="cb2-167"><a href="#cb2-167"></a><span class="co">#| fig-cap: Diagram illustrating the relationship between the code produced by the developer and the shiny docker container utilised by user with a Github cloud conduit.  The developed codebase includes a Shiny R application with R backend, `Dockerfile` (instructions used to assemble a full operating environment) and github workflow file (instructions for building and packaging the docker image on github via `actions`).</span></span>
<span id="cb2-168"><a href="#cb2-168"></a><span class="co">#| file: resources/diagram.tikz </span></span>
<span id="cb2-169"><a href="#cb2-169"></a><span class="co">#| echo: false</span></span>
<span id="cb2-170"><a href="#cb2-170"></a><span class="co">#| eval: true</span></span>
<span id="cb2-171"><a href="#cb2-171"></a><span class="co">#| engine-opts:</span></span>
<span id="cb2-172"><a href="#cb2-172"></a><span class="co">#|   template: &quot;resources/tikz-standalone.tex&quot;</span></span>
<span id="cb2-173"><a href="#cb2-173"></a><span class="in">```</span></span>
<span id="cb2-174"><a href="#cb2-174"></a></span>
<span id="cb2-175"><a href="#cb2-175"></a></span>
<span id="cb2-176"><a href="#cb2-176"></a>  </span>
<span id="cb2-177"><a href="#cb2-177"></a><span class="fu"># Installation</span></span>
<span id="cb2-178"><a href="#cb2-178"></a></span>
<span id="cb2-179"><a href="#cb2-179"></a><span class="fu">## Installing docker desktop</span></span>
<span id="cb2-180"><a href="#cb2-180"></a></span>
<span id="cb2-181"><a href="#cb2-181"></a>To retrieve and run docker containers requires the installation of</span>
<span id="cb2-182"><a href="#cb2-182"></a><span class="co">[</span><span class="ot">Docker Desktop</span><span class="co">](https://www.docker.com/products/docker-desktop/)</span> on</span>
<span id="cb2-183"><a href="#cb2-183"></a>Windows and MacOSx</span>
<span id="cb2-184"><a href="#cb2-184"></a></span>
<span id="cb2-185"><a href="#cb2-185"></a><span class="fu">### Windows</span></span>
<span id="cb2-186"><a href="#cb2-186"></a></span>
<span id="cb2-187"><a href="#cb2-187"></a>The steps for installing Docker Desktop are:</span>
<span id="cb2-188"><a href="#cb2-188"></a></span>
<span id="cb2-189"><a href="#cb2-189"></a><span class="ss">- </span>**Download the Installer:** head to</span>
<span id="cb2-190"><a href="#cb2-190"></a>  <span class="ot">&lt;https://docs.docker.com/desktop/install/windows-install/&gt;</span> and follow</span>
<span id="cb2-191"><a href="#cb2-191"></a>  the instructions for downloading the appropriate installer for your</span>
<span id="cb2-192"><a href="#cb2-192"></a>  Windows version (Home or Pro).</span>
<span id="cb2-193"><a href="#cb2-193"></a></span>
<span id="cb2-194"><a href="#cb2-194"></a><span class="ss">- </span>**Run the Installer:** double-click the downloaded file and follow</span>
<span id="cb2-195"><a href="#cb2-195"></a>  the on-screen instructions from the installation wizard. Accept the</span>
<span id="cb2-196"><a href="#cb2-196"></a>  license agreement and choose your preferred installation location.</span>
<span id="cb2-197"><a href="#cb2-197"></a></span>
<span id="cb2-198"><a href="#cb2-198"></a><span class="ss">- </span>**Configure Resources (Optional):** Docker Desktop might suggest</span>
<span id="cb2-199"><a href="#cb2-199"></a>  allocating some system resources like CPU and memory. These settings</span>
<span id="cb2-200"><a href="#cb2-200"></a>  can be adjusted later, so feel free to use the defaults for now.</span>
<span id="cb2-201"><a href="#cb2-201"></a></span>
<span id="cb2-202"><a href="#cb2-202"></a><span class="ss">- </span>**Start the Docker Engine:** once installed, click the &quot;Start Docker</span>
<span id="cb2-203"><a href="#cb2-203"></a>  Desktop&quot; button. You may see a notification in the taskbar - click</span>
<span id="cb2-204"><a href="#cb2-204"></a>  it to confirm and allow Docker to run in the background.</span>
<span id="cb2-205"><a href="#cb2-205"></a></span>
<span id="cb2-206"><a href="#cb2-206"></a><span class="ss">- </span>**Verification:** open a terminal (or Powershell) and run <span class="in">`docker --version`</span>. </span>
<span id="cb2-207"><a href="#cb2-207"></a>  If all went well, you should see information about the</span>
<span id="cb2-208"><a href="#cb2-208"></a>  installed Docker Engine version.</span>
<span id="cb2-209"><a href="#cb2-209"></a></span>
<span id="cb2-210"><a href="#cb2-210"></a>Additional Tips:</span>
<span id="cb2-211"><a href="#cb2-211"></a></span>
<span id="cb2-212"><a href="#cb2-212"></a><span class="ss">- </span>Ensure Hyper-V (virtualization) is enabled in your BIOS settings for optimal</span>
<span id="cb2-213"><a href="#cb2-213"></a>  performance.</span>
<span id="cb2-214"><a href="#cb2-214"></a></span>
<span id="cb2-215"><a href="#cb2-215"></a></span>
<span id="cb2-216"><a href="#cb2-216"></a><span class="fu">## Installing the and running the app</span></span>
<span id="cb2-217"><a href="#cb2-217"></a></span>
<span id="cb2-218"><a href="#cb2-218"></a>The task of installing and running the app is performed via a single</span>
<span id="cb2-219"><a href="#cb2-219"></a>**deploy script** (<span class="in">`deploy.bat`</span> on Windows or <span class="in">`deploy.sh`</span> on</span>
<span id="cb2-220"><a href="#cb2-220"></a>Linux/MacOSX/wsl). For this to work properly, the deploy script should</span>
<span id="cb2-221"><a href="#cb2-221"></a>be placed in a folder along with a folder (called <span class="in">`input`</span>) that</span>
<span id="cb2-222"><a href="#cb2-222"></a>contains the input datasets (in excel format). This structure is</span>
<span id="cb2-223"><a href="#cb2-223"></a>illustrated below for Windows.</span>
<span id="cb2-224"><a href="#cb2-224"></a></span>
<span id="cb2-225"><a href="#cb2-225"></a><span class="in">```</span></span>
<span id="cb2-226"><a href="#cb2-226"></a><span class="in">\</span></span>
<span id="cb2-227"><a href="#cb2-227"></a><span class="in">|- deploy.bat</span></span>
<span id="cb2-228"><a href="#cb2-228"></a><span class="in">|- input</span></span>
<span id="cb2-229"><a href="#cb2-229"></a><span class="in">   |- dataset1.xlsx</span></span>
<span id="cb2-230"><a href="#cb2-230"></a><span class="in">   |- dataset2.xlsx</span></span>
<span id="cb2-231"><a href="#cb2-231"></a><span class="in">```</span></span>
<span id="cb2-232"><a href="#cb2-232"></a></span>
<span id="cb2-233"><a href="#cb2-233"></a>::: {.callout-note}</span>
<span id="cb2-234"><a href="#cb2-234"></a>In the above illustration, there are two example datasets</span>
<span id="cb2-235"><a href="#cb2-235"></a>(<span class="in">`dataset1.xlsx`</span> and <span class="in">`dataset2.xlsx`</span>). The datasets need NOT be called</span>
<span id="cb2-236"><a href="#cb2-236"></a><span class="in">`dataset1.xlsx`</span>. They can have any name you choose, so long as they</span>
<span id="cb2-237"><a href="#cb2-237"></a>are excel files that adhere to the structure outlined in</span>
<span id="cb2-238"><a href="#cb2-238"></a>@sec-data-requirements.</span>
<span id="cb2-239"><a href="#cb2-239"></a>:::</span>
<span id="cb2-240"><a href="#cb2-240"></a></span>
<span id="cb2-241"><a href="#cb2-241"></a>To set up the above struture:</span>
<span id="cb2-242"><a href="#cb2-242"></a></span>
<span id="cb2-243"><a href="#cb2-243"></a><span class="ss">1. </span>create a new folder on your computer in a location of your choice</span>
<span id="cb2-244"><a href="#cb2-244"></a>   that you are likely to remember and easily locate (e.g. on the</span>
<span id="cb2-245"><a href="#cb2-245"></a>   desktop). Whilst the name of the folder is not important, it is</span>
<span id="cb2-246"><a href="#cb2-246"></a>   recommended that it be named after the project (e.g.</span>
<span id="cb2-247"><a href="#cb2-247"></a>   <span class="in">`darwin_harbour_sediment_monitoring`</span>).</span>
<span id="cb2-248"><a href="#cb2-248"></a></span>
<span id="cb2-249"><a href="#cb2-249"></a><span class="ss">2. </span>download the deploy script from the projects github repository</span>
<span id="cb2-250"><a href="#cb2-250"></a></span>
<span id="cb2-251"><a href="#cb2-251"></a>   a. go to the projects github repository</span>
<span id="cb2-252"><a href="#cb2-252"></a>      (<span class="ot">&lt;https://github.com/open-AIMS/dh_sediment_monitoring.git&gt;</span>) in a</span>
<span id="cb2-253"><a href="#cb2-253"></a>      browser</span>
<span id="cb2-254"><a href="#cb2-254"></a></span>
<span id="cb2-255"><a href="#cb2-255"></a>   b. click on either the <span class="in">`deploy.bat`</span> (Windows) or &#39;deploy.sh`</span>
<span id="cb2-256"><a href="#cb2-256"></a>      (Linux/MacOSX/wsl).</span>
<span id="cb2-257"><a href="#cb2-257"></a></span>
<span id="cb2-258"><a href="#cb2-258"></a>      <span class="al">![](resources/github_deploy_script.png)</span></span>
<span id="cb2-259"><a href="#cb2-259"></a></span>
<span id="cb2-260"><a href="#cb2-260"></a>   c. click on the download button and select the project folder as</span>
<span id="cb2-261"><a href="#cb2-261"></a>      the location to download the file to. If the file is</span>
<span id="cb2-262"><a href="#cb2-262"></a>      automatically downloaded to a downloads folder, move the file to</span>
<span id="cb2-263"><a href="#cb2-263"></a>      the project folder.</span>
<span id="cb2-264"><a href="#cb2-264"></a></span>
<span id="cb2-265"><a href="#cb2-265"></a>      <span class="al">![](resources/github_deploy_script2.png)</span></span>
<span id="cb2-266"><a href="#cb2-266"></a></span>
<span id="cb2-267"><a href="#cb2-267"></a><span class="ss">3. </span>within the project folder, create a folder called <span class="in">`inputs`</span> and</span>
<span id="cb2-268"><a href="#cb2-268"></a>   place all the appropriate data sets into this <span class="in">`inputs`</span> folder</span>
<span id="cb2-269"><a href="#cb2-269"></a></span>
<span id="cb2-270"><a href="#cb2-270"></a>To run the app, navigate inside of the project folder and run</span>
<span id="cb2-271"><a href="#cb2-271"></a>(typically double click) on the deploy script. Upon doing so, you will</span>
<span id="cb2-272"><a href="#cb2-272"></a>be presented with a directory selection window that is prompting for</span>
<span id="cb2-273"><a href="#cb2-273"></a>the path of the project folder. Navigate to and select the project</span>
<span id="cb2-274"><a href="#cb2-274"></a>folder before clicking the &quot;OK&quot; button. Shortly thereafter, the</span>
<span id="cb2-275"><a href="#cb2-275"></a>application will appear in a browser tab.</span>
<span id="cb2-276"><a href="#cb2-276"></a></span>
<span id="cb2-277"><a href="#cb2-277"></a>::: {.callout-note collapse=true}</span>
<span id="cb2-278"><a href="#cb2-278"></a><span class="fu">## More specific information about the `deploy.bat` script</span></span>
<span id="cb2-279"><a href="#cb2-279"></a>The <span class="in">`deploy.bat`</span> script performs the following:</span>
<span id="cb2-280"><a href="#cb2-280"></a></span>
<span id="cb2-281"><a href="#cb2-281"></a><span class="ss">1. </span>defines paths to the project repository and local project folder</span>
<span id="cb2-282"><a href="#cb2-282"></a><span class="ss">2. </span>checks if <span class="in">`docker`</span> is installed and available from the command line</span>
<span id="cb2-283"><a href="#cb2-283"></a>   for the current user</span>
<span id="cb2-284"><a href="#cb2-284"></a><span class="ss">3. </span>checks if <span class="in">`docker`</span> is running</span>
<span id="cb2-285"><a href="#cb2-285"></a><span class="ss">4. </span>query the user for the location of the project folder</span>
<span id="cb2-286"><a href="#cb2-286"></a><span class="ss">5. </span>determine whether there are any updates to the <span class="in">`docker`</span> image and</span>
<span id="cb2-287"><a href="#cb2-287"></a>   if so pull them down</span>
<span id="cb2-288"><a href="#cb2-288"></a><span class="ss">6. </span>run the <span class="in">`docker`</span> container</span>
<span id="cb2-289"><a href="#cb2-289"></a><span class="ss">7. </span>open the shiny app in a browser</span>
<span id="cb2-290"><a href="#cb2-290"></a>:::</span>
<span id="cb2-291"><a href="#cb2-291"></a></span>
<span id="cb2-292"><a href="#cb2-292"></a> </span>
<span id="cb2-293"><a href="#cb2-293"></a><span class="fu"># The Darwin Harbour Sediment Monitoring Program Analysis App</span></span>
<span id="cb2-294"><a href="#cb2-294"></a></span>
<span id="cb2-295"><a href="#cb2-295"></a>{{&lt; include ../md/instructions.md &gt;}}</span>
<span id="cb2-296"><a href="#cb2-296"></a></span>
<span id="cb2-297"><a href="#cb2-297"></a></span>
<span id="cb2-298"><a href="#cb2-298"></a>@fig-diagram2 provides a schematic overview the sequence of</span>
<span id="cb2-299"><a href="#cb2-299"></a>filesystem events that occur during the development, deployment and</span>
<span id="cb2-300"><a href="#cb2-300"></a>running of this app.</span>
<span id="cb2-301"><a href="#cb2-301"></a></span>
<span id="cb2-302"><a href="#cb2-302"></a><span class="ss">1. </span>the developed codebase is pushed to github and if necessary</span>
<span id="cb2-303"><a href="#cb2-303"></a>  continuous integration (github actions) is triggered. The continuous</span>
<span id="cb2-304"><a href="#cb2-304"></a>  integration will re-build and host a docker image as well as rebuild</span>
<span id="cb2-305"><a href="#cb2-305"></a>  the manual.</span>
<span id="cb2-306"><a href="#cb2-306"></a><span class="ss">2. </span>when the client runs the <span class="in">`deploy.bat`</span> (or <span class="in">`deploy.sh`</span>) script, it</span>
<span id="cb2-307"><a href="#cb2-307"></a>   will check whether docker is running and get input from the user</span>
<span id="cb2-308"><a href="#cb2-308"></a>   about the location of the project directory.</span>
<span id="cb2-309"><a href="#cb2-309"></a><span class="ss">3. </span>github will be queried</span>
<span id="cb2-310"><a href="#cb2-310"></a>   to discover if a new docker image is available. If so, then the new</span>
<span id="cb2-311"><a href="#cb2-311"></a>   image will be pulled down locally and run (if docker is runnning). </span>
<span id="cb2-312"><a href="#cb2-312"></a><span class="ss">4. </span>the docker container will be run and this will trigger git within</span>
<span id="cb2-313"><a href="#cb2-313"></a>   the container to pull down the latest version of the codebase from</span>
<span id="cb2-314"><a href="#cb2-314"></a>   github to a temporary repo in the container. As the container is</span>
<span id="cb2-315"><a href="#cb2-315"></a>   starting up, it will mount the project folder so that its contents</span>
<span id="cb2-316"><a href="#cb2-316"></a>   are available to the environment within container and outputs</span>
<span id="cb2-317"><a href="#cb2-317"></a>   produced within the container are available to the host.</span>
<span id="cb2-318"><a href="#cb2-318"></a><span class="ss">5. </span>some of the files in the temporary repo will be copied to a folder</span>
<span id="cb2-319"><a href="#cb2-319"></a>   within the project folder.</span>
<span id="cb2-320"><a href="#cb2-320"></a><span class="ss">6. </span>the shiny app will start up on <span class="in">`port 3838`</span> of the localhost and</span>
<span id="cb2-321"><a href="#cb2-321"></a>   this will be offered to the default browser.</span>
<span id="cb2-322"><a href="#cb2-322"></a><span class="ss">7. </span>as the shiny app progresses through each of the analysis stages,</span>
<span id="cb2-323"><a href="#cb2-323"></a>   more data will be added to various folders of the project</span>
<span id="cb2-324"><a href="#cb2-324"></a>   directory.</span>
<span id="cb2-325"><a href="#cb2-325"></a></span>
<span id="cb2-326"><a href="#cb2-326"></a>quarto-executable-code-5450563D</span>
<span id="cb2-327"><a href="#cb2-327"></a></span>
<span id="cb2-328"><a href="#cb2-328"></a><span class="in">```r</span></span>
<span id="cb2-329"><a href="#cb2-329"></a><span class="co">#| label: fig-diagram2</span></span>
<span id="cb2-330"><a href="#cb2-330"></a><span class="co">#| engine: tikz</span></span>
<span id="cb2-331"><a href="#cb2-331"></a><span class="co">#| fig-cap: Diagram illustrating the sequence of filesystem events that occur during the development, deployment and running of this app.</span></span>
<span id="cb2-332"><a href="#cb2-332"></a><span class="co">#| file: resources/diagram2.tikz </span></span>
<span id="cb2-333"><a href="#cb2-333"></a><span class="co">#| echo: false</span></span>
<span id="cb2-334"><a href="#cb2-334"></a><span class="co">#| eval: true</span></span>
<span id="cb2-335"><a href="#cb2-335"></a><span class="co">#| engine-opts:</span></span>
<span id="cb2-336"><a href="#cb2-336"></a><span class="co">#|   template: &quot;resources/tikz-standalone.tex&quot;</span></span>
<span id="cb2-337"><a href="#cb2-337"></a><span class="in">```</span></span>
<span id="cb2-338"><a href="#cb2-338"></a></span>
<span id="cb2-339"><a href="#cb2-339"></a><span class="fu"># Analysis stages</span></span>
<span id="cb2-340"><a href="#cb2-340"></a></span>
<span id="cb2-341"><a href="#cb2-341"></a><span class="fu">## Stage 2 - obtaining the data</span></span>
<span id="cb2-342"><a href="#cb2-342"></a></span>
<span id="cb2-343"><a href="#cb2-343"></a>At the completion of this stage, the Data sidebar menu and Stage 3</span>
<span id="cb2-344"><a href="#cb2-344"></a>button will become active and the Data page will be populated with the</span>
<span id="cb2-345"><a href="#cb2-345"></a>raw data and available for review.</span>
<span id="cb2-346"><a href="#cb2-346"></a></span>
<span id="cb2-347"><a href="#cb2-347"></a><span class="fu">### Read input info</span></span>
<span id="cb2-348"><a href="#cb2-348"></a></span>
<span id="cb2-349"><a href="#cb2-349"></a>This task seeks to determine what data sources are available and for</span>
<span id="cb2-350"><a href="#cb2-350"></a>those found, stores the names and filetypes discovered. This task will</span>
<span id="cb2-351"><a href="#cb2-351"></a>exclusively search in the <span class="in">`/input`</span> folder of the project directory.</span>
<span id="cb2-352"><a href="#cb2-352"></a></span>
<span id="cb2-353"><a href="#cb2-353"></a><span class="fu">### Read input data</span></span>
<span id="cb2-354"><a href="#cb2-354"></a></span>
<span id="cb2-355"><a href="#cb2-355"></a>This task will sequentially read in each sheet of each data source</span>
<span id="cb2-356"><a href="#cb2-356"></a>(excel file) into a nested list.</span>
<span id="cb2-357"><a href="#cb2-357"></a></span>
<span id="cb2-358"><a href="#cb2-358"></a><span class="fu">### Fix dates</span></span>
<span id="cb2-359"><a href="#cb2-359"></a></span>
<span id="cb2-360"><a href="#cb2-360"></a>This task will ensure that all dates are of the same format.</span>
<span id="cb2-361"><a href="#cb2-361"></a>Spreadsheets often store date/time data in one format and display it</span>
<span id="cb2-362"><a href="#cb2-362"></a>in another format. Consequently, users can be unaware that they have a</span>
<span id="cb2-363"><a href="#cb2-363"></a>mixture of date/time formats present in the same spreadsheet. For the</span>
<span id="cb2-364"><a href="#cb2-364"></a>purpose of data analysis, it is important that all date/time formats</span>
<span id="cb2-365"><a href="#cb2-365"></a>are consistent - this task aims to achieve this.</span>
<span id="cb2-366"><a href="#cb2-366"></a></span>
<span id="cb2-367"><a href="#cb2-367"></a><span class="fu">### Validate input data</span></span>
<span id="cb2-368"><a href="#cb2-368"></a></span>
<span id="cb2-369"><a href="#cb2-369"></a>This task performs data validation in accordance with the rules set</span>
<span id="cb2-370"><a href="#cb2-370"></a>out in the following section.</span>
<span id="cb2-371"><a href="#cb2-371"></a></span>
<span id="cb2-372"><a href="#cb2-372"></a><span class="fu">#### Data requirements {#sec-data-requirements}</span></span>
<span id="cb2-373"><a href="#cb2-373"></a></span>
<span id="cb2-374"><a href="#cb2-374"></a>{{&lt; include ../md/raw_data.md &gt;}}</span>
<span id="cb2-375"><a href="#cb2-375"></a></span>
<span id="cb2-376"><a href="#cb2-376"></a><span class="fu">### Make spatial data</span></span>
<span id="cb2-377"><a href="#cb2-377"></a></span>
<span id="cb2-378"><a href="#cb2-378"></a>This task will compile a set of spatial artifacts from GIS shapefiles</span>
<span id="cb2-379"><a href="#cb2-379"></a>of Darwin Harbour. These spatial artifacts will be used to spatially</span>
<span id="cb2-380"><a href="#cb2-380"></a>join the sediment data in order to assign spatial scales such as Zones</span>
<span id="cb2-381"><a href="#cb2-381"></a>and Areas to the data. They will also be used to facilitate mapping of</span>
<span id="cb2-382"><a href="#cb2-382"></a>the data. The shapefiles used in this task are built into the app. If</span>
<span id="cb2-383"><a href="#cb2-383"></a>there is a need to change these, please contact the app author.</span>
<span id="cb2-384"><a href="#cb2-384"></a></span>
<span id="cb2-385"><a href="#cb2-385"></a><span class="fu">### Make spatial lookup</span></span>
<span id="cb2-386"><a href="#cb2-386"></a></span>
<span id="cb2-387"><a href="#cb2-387"></a>This stage creates a lookup table that relates each of the spatial</span>
<span id="cb2-388"><a href="#cb2-388"></a>scales to one another. This lookup is used to inject the spatial</span>
<span id="cb2-389"><a href="#cb2-389"></a>information into the data and modelled derivatives after they are</span>
<span id="cb2-390"><a href="#cb2-390"></a>created and in so doing prevents the need to spatially join the data</span>
<span id="cb2-391"><a href="#cb2-391"></a>each time it is required.</span>
<span id="cb2-392"><a href="#cb2-392"></a></span>
<span id="cb2-393"><a href="#cb2-393"></a><span class="fu">## Stage 3 - processing the data</span></span>
<span id="cb2-394"><a href="#cb2-394"></a></span>
<span id="cb2-395"><a href="#cb2-395"></a>At the completion of this stage, the Stage 4 button will become active</span>
<span id="cb2-396"><a href="#cb2-396"></a>and the Processed Data sub-page of the Data page will be populated</span>
<span id="cb2-397"><a href="#cb2-397"></a>with the processed data and available for review.</span>
<span id="cb2-398"><a href="#cb2-398"></a></span>
<span id="cb2-399"><a href="#cb2-399"></a><span class="fu">### Retrieve data</span></span>
<span id="cb2-400"><a href="#cb2-400"></a></span>
<span id="cb2-401"><a href="#cb2-401"></a>This task literally just reads in the data stored at the end of the</span>
<span id="cb2-402"><a href="#cb2-402"></a>previous stage.</span>
<span id="cb2-403"><a href="#cb2-403"></a></span>
<span id="cb2-404"><a href="#cb2-404"></a><span class="fu">### Apply LoRs</span></span>
<span id="cb2-405"><a href="#cb2-405"></a></span>
<span id="cb2-406"><a href="#cb2-406"></a>This task applies rules for the presence of data that are below Limit</span>
<span id="cb2-407"><a href="#cb2-407"></a>of Reporting (LoR). In the data, LoR values are indicated by the</span>
<span id="cb2-408"><a href="#cb2-408"></a>presence of a <span class="in">`&lt;`</span> symbol. There are two ways available for handling</span>
<span id="cb2-409"><a href="#cb2-409"></a>LoR values.</span>
<span id="cb2-410"><a href="#cb2-410"></a></span>
<span id="cb2-411"><a href="#cb2-411"></a><span class="ss">1. </span>Traditionally, values that represent Limit of Reporting (LoR) were</span>
<span id="cb2-412"><a href="#cb2-412"></a>   replaced with half the LoR value. For example, a value of &lt;0.02</span>
<span id="cb2-413"><a href="#cb2-413"></a>   would be replaced with 0.01.</span>
<span id="cb2-414"><a href="#cb2-414"></a><span class="ss">2. </span>However, modern statistical analyses have more appropriate ways of</span>
<span id="cb2-415"><a href="#cb2-415"></a>   incorporating LoR information. Rather than arbitrarily replace</span>
<span id="cb2-416"><a href="#cb2-416"></a>   values with half the LoR, we retain their value and flag them as</span>
<span id="cb2-417"><a href="#cb2-417"></a>   censored and allow the statistical properties of disbutions to</span>
<span id="cb2-418"><a href="#cb2-418"></a>   handle them more naturally.</span>
<span id="cb2-419"><a href="#cb2-419"></a></span>
<span id="cb2-420"><a href="#cb2-420"></a>In either case, a LoR flag is then attached to any value that was</span>
<span id="cb2-421"><a href="#cb2-421"></a>deemed LoR.</span>
<span id="cb2-422"><a href="#cb2-422"></a></span>
<span id="cb2-423"><a href="#cb2-423"></a><span class="fu">### Pivot data</span></span>
<span id="cb2-424"><a href="#cb2-424"></a></span>
<span id="cb2-425"><a href="#cb2-425"></a>This task pivots (reshapes) the data from wide to long format. Wide</span>
<span id="cb2-426"><a href="#cb2-426"></a>format, in which each row represents a single site/time and each</span>
<span id="cb2-427"><a href="#cb2-427"></a>variable is in its own column is a convenient way to assemble data</span>
<span id="cb2-428"><a href="#cb2-428"></a>(particularly as it permits the user to easily identify missing</span>
<span id="cb2-429"><a href="#cb2-429"></a>values). However, data analysis requires that each individual record</span>
<span id="cb2-430"><a href="#cb2-430"></a>(observation) be in its own row.</span>
<span id="cb2-431"><a href="#cb2-431"></a></span>
<span id="cb2-432"><a href="#cb2-432"></a><span class="fu">### Join metadata</span></span>
<span id="cb2-433"><a href="#cb2-433"></a></span>
<span id="cb2-434"><a href="#cb2-434"></a>This task joins (merges) the each of the main sediment data sheets</span>
<span id="cb2-435"><a href="#cb2-435"></a>(metals, hydrocarbons and total carbons) with the metadata sheet.</span>
<span id="cb2-436"><a href="#cb2-436"></a></span>
<span id="cb2-437"><a href="#cb2-437"></a><span class="fu">### Make sample key</span></span>
<span id="cb2-438"><a href="#cb2-438"></a></span>
<span id="cb2-439"><a href="#cb2-439"></a>This task generates a unique key to uniquely identify each individual</span>
<span id="cb2-440"><a href="#cb2-440"></a>record by combining information about the Site_ID, acquire date/time</span>
<span id="cb2-441"><a href="#cb2-441"></a>and the part of the sample ID that indicates whether or not the sample</span>
<span id="cb2-442"><a href="#cb2-442"></a>was a replicate or duplicate.</span>
<span id="cb2-443"><a href="#cb2-443"></a></span>
<span id="cb2-444"><a href="#cb2-444"></a><span class="fu">### Collate data</span></span>
<span id="cb2-445"><a href="#cb2-445"></a></span>
<span id="cb2-446"><a href="#cb2-446"></a>This task combines all the data sources (years) and types (metals,</span>
<span id="cb2-447"><a href="#cb2-447"></a>hydrocarbons, total carbons) together into a single data set. At the</span>
<span id="cb2-448"><a href="#cb2-448"></a>same time, it also creates some additional fields:</span>
<span id="cb2-449"><a href="#cb2-449"></a></span>
<span id="cb2-450"><a href="#cb2-450"></a><span class="ss">- </span><span class="in">`Year_cal`</span> a field that represents the calendar year in which the</span>
<span id="cb2-451"><a href="#cb2-451"></a>  sample was collected</span>
<span id="cb2-452"><a href="#cb2-452"></a><span class="ss">- </span><span class="in">`Year_fiscal`</span> a field that represents the fiscal year in which the</span>
<span id="cb2-453"><a href="#cb2-453"></a>  sample was collected</span>
<span id="cb2-454"><a href="#cb2-454"></a><span class="ss">- </span><span class="in">`Year_water`</span> a field that represents the water year (defined as 1st</span>
<span id="cb2-455"><a href="#cb2-455"></a>  Oct through to 30 Sept) in which the sample was collected</span>
<span id="cb2-456"><a href="#cb2-456"></a><span class="ss">- </span><span class="in">`Baseline`</span> a field that represents whether the observation is</span>
<span id="cb2-457"><a href="#cb2-457"></a>  considered a &quot;Baseline&quot; observation or not</span>
<span id="cb2-458"><a href="#cb2-458"></a><span class="ss">- </span><span class="in">`Replicate_flag`</span> a field that represents whether the observation is</span>
<span id="cb2-459"><a href="#cb2-459"></a>  a replicate</span>
<span id="cb2-460"><a href="#cb2-460"></a><span class="ss">- </span><span class="in">`Duplicate_flag`</span> a field that represents whether the observation is</span>
<span id="cb2-461"><a href="#cb2-461"></a>  a duplicate</span>
<span id="cb2-462"><a href="#cb2-462"></a></span>
<span id="cb2-463"><a href="#cb2-463"></a><span class="fu">### Incorporate spatial data</span></span>
<span id="cb2-464"><a href="#cb2-464"></a></span>
<span id="cb2-465"><a href="#cb2-465"></a>This task uses the spatial artifacts created in the previous stage to</span>
<span id="cb2-466"><a href="#cb2-466"></a>add spatial information to the data. This spatial information includes</span>
<span id="cb2-467"><a href="#cb2-467"></a>the Zone, Area and Site that each observation belongs to.</span>
<span id="cb2-468"><a href="#cb2-468"></a></span>
<span id="cb2-469"><a href="#cb2-469"></a><span class="fu">### Tidy data</span></span>
<span id="cb2-470"><a href="#cb2-470"></a></span>
<span id="cb2-471"><a href="#cb2-471"></a>This task creates a new field <span class="in">`Site`</span> that acts as a unique identifier</span>
<span id="cb2-472"><a href="#cb2-472"></a>of a sampling location over time. This field is created by copying the</span>
<span id="cb2-473"><a href="#cb2-473"></a><span class="in">`IBSM_site`</span> field (if it is not empty), otherwise the <span class="in">`Site_ID`</span> field</span>
<span id="cb2-474"><a href="#cb2-474"></a>is used. This task also removes any fields that are no longer</span>
<span id="cb2-475"><a href="#cb2-475"></a>required.</span>
<span id="cb2-476"><a href="#cb2-476"></a></span>
<span id="cb2-477"><a href="#cb2-477"></a><span class="fu">### Standardise data</span></span>
<span id="cb2-478"><a href="#cb2-478"></a></span>
<span id="cb2-479"><a href="#cb2-479"></a>quarto-executable-code-5450563D</span>
<span id="cb2-480"><a href="#cb2-480"></a></span>
<span id="cb2-481"><a href="#cb2-481"></a><span class="in">```r</span></span>
<span id="cb2-482"><a href="#cb2-482"></a><span class="co">#| label: fig-standardisations</span></span>
<span id="cb2-483"><a href="#cb2-483"></a><span class="co">#| engine: tikz</span></span>
<span id="cb2-484"><a href="#cb2-484"></a><span class="co">#| fig-cap: Diagram illustrating the standardisation (normalisation) rules applied to each variable.  In each case, the text in blue represents the appropriate divisor used in the standardisation.</span></span>
<span id="cb2-485"><a href="#cb2-485"></a><span class="co">#| file: resources/standardisations.tikz </span></span>
<span id="cb2-486"><a href="#cb2-486"></a><span class="co">#| echo: false</span></span>
<span id="cb2-487"><a href="#cb2-487"></a><span class="co">#| eval: true</span></span>
<span id="cb2-488"><a href="#cb2-488"></a><span class="co">#| engine-opts:</span></span>
<span id="cb2-489"><a href="#cb2-489"></a><span class="co">#|   template: &quot;resources/tikz-standalone.tex&quot;</span></span>
<span id="cb2-490"><a href="#cb2-490"></a><span class="in">```</span></span>
<span id="cb2-491"><a href="#cb2-491"></a>  </span>
<span id="cb2-492"><a href="#cb2-492"></a><span class="fu">### Create site lookup</span></span>
<span id="cb2-493"><a href="#cb2-493"></a></span>
<span id="cb2-494"><a href="#cb2-494"></a>This task creates a lookup that maps sites to zones.</span>
<span id="cb2-495"><a href="#cb2-495"></a></span>
<span id="cb2-496"><a href="#cb2-496"></a><span class="fu">## Stage 4 - Exploratory data analysis</span></span>
<span id="cb2-497"><a href="#cb2-497"></a></span>
<span id="cb2-498"><a href="#cb2-498"></a>At the completion of this stage, the Exploratory Data Analysis menu</span>
<span id="cb2-499"><a href="#cb2-499"></a>and Stage 5 button will become active and the Exploratory Data</span>
<span id="cb2-500"><a href="#cb2-500"></a>Analysis page will be populated with the a range of exploratory</span>
<span id="cb2-501"><a href="#cb2-501"></a>figures. This Stage involves numerous tasks that each prepare the data</span>
<span id="cb2-502"><a href="#cb2-502"></a>in formats conducive to the production of the figures while navigating</span>
<span id="cb2-503"><a href="#cb2-503"></a>the Exploratory Data Analysis page.</span>
<span id="cb2-504"><a href="#cb2-504"></a></span>
<span id="cb2-505"><a href="#cb2-505"></a><span class="fu">## Stage 5 - Temporal analysis</span></span>
<span id="cb2-506"><a href="#cb2-506"></a></span>
<span id="cb2-507"><a href="#cb2-507"></a>At the completion of this stage, the Analysis menu will become active</span>
<span id="cb2-508"><a href="#cb2-508"></a>and the Analysis page will be populated with the a range of modelled</span>
<span id="cb2-509"><a href="#cb2-509"></a>outputs.</span>
<span id="cb2-510"><a href="#cb2-510"></a></span>
<span id="cb2-511"><a href="#cb2-511"></a>The temporal analyses essentially involve the fitting of separate</span>
<span id="cb2-512"><a href="#cb2-512"></a>Bayesian Hierarchical models <span class="co">[</span><span class="ot">@Gelman-2007-2007</span><span class="co">]</span> to the full time</span>
<span id="cb2-513"><a href="#cb2-513"></a>series of all sites within each focal Zone. From such models (outline</span>
<span id="cb2-514"><a href="#cb2-514"></a>below), site and zone level modelled trends can be inferred and</span>
<span id="cb2-515"><a href="#cb2-515"></a>thereafter aggregated up to Area and Whole of Harbour level trends as</span>
<span id="cb2-516"><a href="#cb2-516"></a>well.</span>
<span id="cb2-517"><a href="#cb2-517"></a></span>
<span id="cb2-518"><a href="#cb2-518"></a>The general form of the models employed is as follows:</span>
<span id="cb2-519"><a href="#cb2-519"></a></span>
<span id="cb2-520"><a href="#cb2-520"></a>$$</span>
<span id="cb2-521"><a href="#cb2-521"></a>\begin{aligned}</span>
<span id="cb2-522"><a href="#cb2-522"></a>y_{i,s} &amp;\sim{} \Gamma(\mu_{i,s}, \phi)<span class="sc">\\</span></span>
<span id="cb2-523"><a href="#cb2-523"></a>log(\mu_{i,s}) &amp;= (\beta_0 + \gamma_{s<span class="co">[</span><span class="ot">i</span><span class="co">]</span>,0}) + \sum_{j=1}^nT_{<span class="co">[</span><span class="ot">i</span><span class="co">]</span>,j}.(\beta_j + \gamma_{s<span class="co">[</span><span class="ot">i</span><span class="co">]</span>,j]})<span class="sc">\\</span></span>
<span id="cb2-524"><a href="#cb2-524"></a>\phi&amp;\sim\Gamma(0.01, 0.01)<span class="sc">\\</span></span>
<span id="cb2-525"><a href="#cb2-525"></a>\beta_0&amp;\sim{}\mathit{t}(3, \alpha_1, \alpha_2)<span class="sc">\\</span></span>
<span id="cb2-526"><a href="#cb2-526"></a>\beta_{<span class="co">[</span><span class="ot">1,n</span><span class="co">]</span>}&amp;\sim{}\mathit{t}(3, 0, \alpha_3)<span class="sc">\\</span></span>
<span id="cb2-527"><a href="#cb2-527"></a>\gamma_{<span class="co">[</span><span class="ot">1..p</span><span class="co">]</span>}&amp;\sim{}MVN(0, \boldsymbol{\Sigma_s})<span class="sc">\\</span></span>
<span id="cb2-528"><a href="#cb2-528"></a>\boldsymbol{\Sigma_s} &amp;=</span>
<span id="cb2-529"><a href="#cb2-529"></a>{\begin{pmatrix}</span>
<span id="cb2-530"><a href="#cb2-530"></a>\sigma_{s_1}^2 &amp; \rho_s \sigma_{s_1} \sigma_{s_2} &amp; \rho_s \sigma_{s_1} \sigma_{s_3}<span class="sc">\\</span></span>
<span id="cb2-531"><a href="#cb2-531"></a>\rho_s \sigma_{s_1} \sigma_{s_2} &amp; \sigma_{s_2}^2 &amp; \rho_s \sigma_{s_2} \sigma_{s_3}<span class="sc">\\</span></span>
<span id="cb2-532"><a href="#cb2-532"></a>\rho_s \sigma_{s_1} \sigma_{s_3}  &amp; \rho_s \sigma_{s_2} \sigma_{s_3} &amp; \sigma_{s_3}^2</span>
<span id="cb2-533"><a href="#cb2-533"></a>\end{pmatrix}}<span class="sc">\\</span></span>
<span id="cb2-534"><a href="#cb2-534"></a>\sigma_{s<span class="co">[</span><span class="ot">1,2,3</span><span class="co">]</span>} &amp;\sim \mathit{t}(3, 0, \alpha_2)<span class="sc">\\</span></span>
<span id="cb2-535"><a href="#cb2-535"></a>\rho_s &amp;\sim \mathit{LKJcorr}(1)<span class="sc">\\</span></span>
<span id="cb2-536"><a href="#cb2-536"></a>\end{aligned}</span>
<span id="cb2-537"><a href="#cb2-537"></a>$$</span>
<span id="cb2-538"><a href="#cb2-538"></a></span>
<span id="cb2-539"><a href="#cb2-539"></a>The $i_{th}$ value ($y$) from the $s_{th}$ site was assumed to be</span>
<span id="cb2-540"><a href="#cb2-540"></a>drawn from a gamma ($\Gamma$) distribution parameterised by a mean</span>
<span id="cb2-541"><a href="#cb2-541"></a>($\mu_{i,s}$) and dispersion ($\phi$) respectively. The (natural log)</span>
<span id="cb2-542"><a href="#cb2-542"></a>expected means were described by a linear model that included an</span>
<span id="cb2-543"><a href="#cb2-543"></a>intercept ($\beta_0$), varying effects of site ($\gamma_{s,0}$) and</span>
<span id="cb2-544"><a href="#cb2-544"></a>annual changes in value ($\gamma_{s,j}$) as well as the population</span>
<span id="cb2-545"><a href="#cb2-545"></a>effects ($\beta_1$) of year ($T$). Weakly informative flat-t (3 df)</span>
<span id="cb2-546"><a href="#cb2-546"></a>priors were applied to the intercept and all population effect</span>
<span id="cb2-547"><a href="#cb2-547"></a>parameters. The values ($\alpha_1$, $alpha_2$ and $\alpha_3$) used to</span>
<span id="cb2-548"><a href="#cb2-548"></a>define the weakly informative priors were developed from simple</span>
<span id="cb2-549"><a href="#cb2-549"></a>summary statistics of the raw data. A weakly informative gamma prior</span>
<span id="cb2-550"><a href="#cb2-550"></a>was applied to the dispersion parameter. The varying effects were</span>
<span id="cb2-551"><a href="#cb2-551"></a>assumed to follow a multivariate normal with a site-specific</span>
<span id="cb2-552"><a href="#cb2-552"></a>covariance structure whose variances follow a weakly informative flat</span>
<span id="cb2-553"><a href="#cb2-553"></a>t distribution and whose correlation follows a LJK distribution with</span>
<span id="cb2-554"><a href="#cb2-554"></a>parameter of 1.</span>
<span id="cb2-555"><a href="#cb2-555"></a></span>
<span id="cb2-556"><a href="#cb2-556"></a>When the data include values that are below the limit of detection,</span>
<span id="cb2-557"><a href="#cb2-557"></a>the model outlined above is modified so as to apply left censoring.</span>
<span id="cb2-558"><a href="#cb2-558"></a></span>
<span id="cb2-559"><a href="#cb2-559"></a>All Bayesian models were fit using the <span class="in">`brms`</span> <span class="co">[</span><span class="ot">@brms</span><span class="co">]</span> package within</span>
<span id="cb2-560"><a href="#cb2-560"></a>the R Graphical and Statistical Environment <span class="co">[</span><span class="ot">@R-2024</span><span class="co">]</span>. All models had</span>
<span id="cb2-561"><a href="#cb2-561"></a>an adaptive delta of 0.95 and had three chains, each with 5000</span>
<span id="cb2-562"><a href="#cb2-562"></a>no-u-turn (NUTS) iterations, a warmup of 1000 and a thinning rate of</span>
<span id="cb2-563"><a href="#cb2-563"></a>5.</span>
<span id="cb2-564"><a href="#cb2-564"></a></span>
<span id="cb2-565"><a href="#cb2-565"></a>Separate models are fit to each variable, for each appropriate</span>
<span id="cb2-566"><a href="#cb2-566"></a>standardisation type, for each zone. At the time of writing this</span>
<span id="cb2-567"><a href="#cb2-567"></a>manual, this equates to nearly 300 models.</span>
<span id="cb2-568"><a href="#cb2-568"></a></span>
<span id="cb2-569"><a href="#cb2-569"></a><span class="fu">### Retrieve data</span></span>
<span id="cb2-570"><a href="#cb2-570"></a></span>
<span id="cb2-571"><a href="#cb2-571"></a>This task literally just reads in the data stored at the end of the</span>
<span id="cb2-572"><a href="#cb2-572"></a>previous stage.</span>
<span id="cb2-573"><a href="#cb2-573"></a></span>
<span id="cb2-574"><a href="#cb2-574"></a><span class="fu">### Prepare data</span></span>
<span id="cb2-575"><a href="#cb2-575"></a></span>
<span id="cb2-576"><a href="#cb2-576"></a>This task ensures that the data are formatted and packaged up into</span>
<span id="cb2-577"><a href="#cb2-577"></a>sets associated with each individual model.</span>
<span id="cb2-578"><a href="#cb2-578"></a></span>
<span id="cb2-579"><a href="#cb2-579"></a><span class="fu">### Prepare priors</span></span>
<span id="cb2-580"><a href="#cb2-580"></a></span>
<span id="cb2-581"><a href="#cb2-581"></a>This task is responsible for defining weakly informative priors on all</span>
<span id="cb2-582"><a href="#cb2-582"></a>parameters for each model. The priors associated with the model for a</span>
<span id="cb2-583"><a href="#cb2-583"></a>specific variable/zone were developed by taking simple summary</span>
<span id="cb2-584"><a href="#cb2-584"></a>statistics of the mean, median, standard deviation and median absolute</span>
<span id="cb2-585"><a href="#cb2-585"></a>deviation of the log of the values conditional on year.</span>
<span id="cb2-586"><a href="#cb2-586"></a></span>
<span id="cb2-587"><a href="#cb2-587"></a><span class="ss">- </span>$\alpha_1$ was taken from the median of the log values from the</span>
<span id="cb2-588"><a href="#cb2-588"></a>  first sampling year. This was used as the mean of the model</span>
<span id="cb2-589"><a href="#cb2-589"></a>  intercept ($\beta_0$) prior</span>
<span id="cb2-590"><a href="#cb2-590"></a><span class="ss">- </span>$\alpha_2$ was taken from the maximum of the median absolute deviations of log</span>
<span id="cb2-591"><a href="#cb2-591"></a>  values for each sampling year. This was used as the standard</span>
<span id="cb2-592"><a href="#cb2-592"></a>  deviation for the model intercept ($\beta_0$) as well as the standard</span>
<span id="cb2-593"><a href="#cb2-593"></a>  deviation for the variances ($\sigma_s$) of the varying effects.</span>
<span id="cb2-594"><a href="#cb2-594"></a><span class="ss">- </span>$\alpha_3$ was taken as the maximum of the difference in mean log</span>
<span id="cb2-595"><a href="#cb2-595"></a>  values between years and this was used to inform the standard</span>
<span id="cb2-596"><a href="#cb2-596"></a>  deviation of the population effect ($\beta$) priors.</span>
<span id="cb2-597"><a href="#cb2-597"></a></span>
<span id="cb2-598"><a href="#cb2-598"></a><span class="fu">### Prepare model template</span></span>
<span id="cb2-599"><a href="#cb2-599"></a></span>
<span id="cb2-600"><a href="#cb2-600"></a>This task essentially involves compiling a single simple model to use</span>
<span id="cb2-601"><a href="#cb2-601"></a>as a template for most other models. Model compilation consumes</span>
<span id="cb2-602"><a href="#cb2-602"></a>approximately 40 seconds of time prior to the model running. Since</span>
<span id="cb2-603"><a href="#cb2-603"></a>most of the models are the same (only the priors and the data differ),</span>
<span id="cb2-604"><a href="#cb2-604"></a>and this project requires the fitting of a very large number of</span>
<span id="cb2-605"><a href="#cb2-605"></a>models, the use of a pre-compiled template can speed up the overall</span>
<span id="cb2-606"><a href="#cb2-606"></a>modell fitting process dramatically.</span>
<span id="cb2-607"><a href="#cb2-607"></a></span>
<span id="cb2-608"><a href="#cb2-608"></a><span class="fu">### Fit models</span></span>
<span id="cb2-609"><a href="#cb2-609"></a></span>
<span id="cb2-610"><a href="#cb2-610"></a>This task involves fitting all combinations of the models. As each new</span>
<span id="cb2-611"><a href="#cb2-611"></a>model is fit, the **Model Logs* pane of the **Dashboard** page will be</span>
<span id="cb2-612"><a href="#cb2-612"></a>updated with a running progress (model number out of a total),</span>
<span id="cb2-613"><a href="#cb2-613"></a>zone/variable/standardisation name along with a message indicating</span>
<span id="cb2-614"><a href="#cb2-614"></a>whether the model was run or retrieved from a previous run. Each</span>
<span id="cb2-615"><a href="#cb2-615"></a>single model is expected to take approximately 1 minute to run</span>
<span id="cb2-616"><a href="#cb2-616"></a>(depending on the clock speed of the computer) so adjust your</span>
<span id="cb2-617"><a href="#cb2-617"></a>expectations accordingly.</span>
<span id="cb2-618"><a href="#cb2-618"></a></span>
<span id="cb2-619"><a href="#cb2-619"></a><span class="fu">### Validate models</span></span>
<span id="cb2-620"><a href="#cb2-620"></a></span>
<span id="cb2-621"><a href="#cb2-621"></a>This task will perform a range of model validation checks and assign</span>
<span id="cb2-622"><a href="#cb2-622"></a>flags against models that display sub-optimal characteristics. Similar</span>
<span id="cb2-623"><a href="#cb2-623"></a>to the model fitting task, the status of validation can be tracked in</span>
<span id="cb2-624"><a href="#cb2-624"></a>the **Model Log** pane of the **Dashboard** page. Details of the</span>
<span id="cb2-625"><a href="#cb2-625"></a>validations performed are given in section @sec-validation.</span>
<span id="cb2-626"><a href="#cb2-626"></a></span>
<span id="cb2-627"><a href="#cb2-627"></a><span class="fu">### Compile zone results</span></span>
<span id="cb2-628"><a href="#cb2-628"></a></span>
<span id="cb2-629"><a href="#cb2-629"></a>This task will extract posteriors and summaries for model derived cell</span>
<span id="cb2-630"><a href="#cb2-630"></a>means (estimates for each year) for each zone along with effects</span>
<span id="cb2-631"><a href="#cb2-631"></a>(comparisons between sets of years). With respect to the effects, the</span>
<span id="cb2-632"><a href="#cb2-632"></a>comparisons are:</span>
<span id="cb2-633"><a href="#cb2-633"></a></span>
<span id="cb2-634"><a href="#cb2-634"></a><span class="ss">- </span>Baseline to each subsequent year</span>
<span id="cb2-635"><a href="#cb2-635"></a><span class="ss">- </span>Most recent year to the year prior to that</span>
<span id="cb2-636"><a href="#cb2-636"></a></span>
<span id="cb2-637"><a href="#cb2-637"></a>The full posteriors of each of the above are stored in files to be</span>
<span id="cb2-638"><a href="#cb2-638"></a>accessed from the **Analysis** page.</span>
<span id="cb2-639"><a href="#cb2-639"></a></span>
<span id="cb2-640"><a href="#cb2-640"></a><span class="fu">### Collect zone results</span></span>
<span id="cb2-641"><a href="#cb2-641"></a></span>
<span id="cb2-642"><a href="#cb2-642"></a>This task collects file pointers across all models together into a</span>
<span id="cb2-643"><a href="#cb2-643"></a>single file for more convenient access in the **Data** page.</span>
<span id="cb2-644"><a href="#cb2-644"></a></span>
<span id="cb2-645"><a href="#cb2-645"></a><span class="fu">### Compile site results</span></span>
<span id="cb2-646"><a href="#cb2-646"></a></span>
<span id="cb2-647"><a href="#cb2-647"></a>Similar to the **Compile zone results** this task extracts posteriors</span>
<span id="cb2-648"><a href="#cb2-648"></a>and summaries for model derived cell means (estimates for each year)</span>
<span id="cb2-649"><a href="#cb2-649"></a>for each site along with effects (comparisons between sets of years).</span>
<span id="cb2-650"><a href="#cb2-650"></a>With respect to the effects, the comparisons are:</span>
<span id="cb2-651"><a href="#cb2-651"></a></span>
<span id="cb2-652"><a href="#cb2-652"></a><span class="ss">- </span>Baseline to each subsequent year</span>
<span id="cb2-653"><a href="#cb2-653"></a><span class="ss">- </span>Most recent year to the year prior to that</span>
<span id="cb2-654"><a href="#cb2-654"></a></span>
<span id="cb2-655"><a href="#cb2-655"></a>The full posteriors of each of the above are stored in files to be</span>
<span id="cb2-656"><a href="#cb2-656"></a>accessed from the **Analysis** page.</span>
<span id="cb2-657"><a href="#cb2-657"></a></span>
<span id="cb2-658"><a href="#cb2-658"></a><span class="fu">### Collect site results</span></span>
<span id="cb2-659"><a href="#cb2-659"></a></span>
<span id="cb2-660"><a href="#cb2-660"></a>This task collects file pointers across all models together into a</span>
<span id="cb2-661"><a href="#cb2-661"></a>single file for more convenient access in the **Data** page.</span>
<span id="cb2-662"><a href="#cb2-662"></a></span>
<span id="cb2-663"><a href="#cb2-663"></a><span class="fu">### Compile area results</span></span>
<span id="cb2-664"><a href="#cb2-664"></a></span>
<span id="cb2-665"><a href="#cb2-665"></a>This task aggregates the zone level posteriors together before</span>
<span id="cb2-666"><a href="#cb2-666"></a>re-calculating the cell means and effects.</span>
<span id="cb2-667"><a href="#cb2-667"></a></span>
<span id="cb2-668"><a href="#cb2-668"></a><span class="fu">### Collect area results</span></span>
<span id="cb2-669"><a href="#cb2-669"></a></span>
<span id="cb2-670"><a href="#cb2-670"></a>This task collects file pointers across all models together into a</span>
<span id="cb2-671"><a href="#cb2-671"></a>single file for more convenient access in the **Data** page.</span>
<span id="cb2-672"><a href="#cb2-672"></a></span>
<span id="cb2-673"><a href="#cb2-673"></a>Finally all site, zone and area results are concatenated together into</span>
<span id="cb2-674"><a href="#cb2-674"></a>a single file.</span>
<span id="cb2-675"><a href="#cb2-675"></a></span>
<span id="cb2-676"><a href="#cb2-676"></a><span class="fu"># App pages</span></span>
<span id="cb2-677"><a href="#cb2-677"></a></span>
<span id="cb2-678"><a href="#cb2-678"></a><span class="fu">## Landing page {#sec-landing}</span></span>
<span id="cb2-679"><a href="#cb2-679"></a></span>
<span id="cb2-680"><a href="#cb2-680"></a>{{&lt; include ../md/settings.md &gt;}}</span>
<span id="cb2-681"><a href="#cb2-681"></a></span>
<span id="cb2-682"><a href="#cb2-682"></a><span class="fu">## Dashboard {#sec-dashboard}</span></span>
<span id="cb2-683"><a href="#cb2-683"></a></span>
<span id="cb2-684"><a href="#cb2-684"></a>{{&lt; include ../md/dashboard.md &gt;}}</span>
<span id="cb2-685"><a href="#cb2-685"></a></span>
<span id="cb2-686"><a href="#cb2-686"></a><span class="fu">## Data {#sec-data}</span></span>
<span id="cb2-687"><a href="#cb2-687"></a></span>
<span id="cb2-688"><a href="#cb2-688"></a>The Data page comprises two panels or subpages accessable by tabs</span>
<span id="cb2-689"><a href="#cb2-689"></a>named &quot;Raw data&quot; and &quot;Processed data&quot; at the top of the page.</span>
<span id="cb2-690"><a href="#cb2-690"></a></span>
<span id="cb2-691"><a href="#cb2-691"></a>::: {.callout-note}</span>
<span id="cb2-692"><a href="#cb2-692"></a>The contents of the Processed data subpage will not be revealed until</span>
<span id="cb2-693"><a href="#cb2-693"></a>the completion of Stage 3.</span>
<span id="cb2-694"><a href="#cb2-694"></a>:::</span>
<span id="cb2-695"><a href="#cb2-695"></a></span>
<span id="cb2-696"><a href="#cb2-696"></a></span>
<span id="cb2-697"><a href="#cb2-697"></a><span class="fu">### Raw data panel</span></span>
<span id="cb2-698"><a href="#cb2-698"></a></span>
<span id="cb2-699"><a href="#cb2-699"></a>{{&lt; include ../md/data_raw_instructions.md &gt;}}</span>
<span id="cb2-700"><a href="#cb2-700"></a></span>
<span id="cb2-701"><a href="#cb2-701"></a><span class="fu">### Processed data panel</span></span>
<span id="cb2-702"><a href="#cb2-702"></a></span>
<span id="cb2-703"><a href="#cb2-703"></a>The Processed data panel displays the first 10 rows of the complete,</span>
<span id="cb2-704"><a href="#cb2-704"></a>compiled and processed data set. Descriptions of each field of these</span>
<span id="cb2-705"><a href="#cb2-705"></a>data are provided in the table below.</span>
<span id="cb2-706"><a href="#cb2-706"></a></span>
<span id="cb2-707"><a href="#cb2-707"></a>::: {.callout-note}</span>
<span id="cb2-708"><a href="#cb2-708"></a>This panel will not become active until the completion of Stage 3.</span>
<span id="cb2-709"><a href="#cb2-709"></a>:::</span>
<span id="cb2-710"><a href="#cb2-710"></a></span>
<span id="cb2-711"><a href="#cb2-711"></a>{{&lt; include ../md/processed_data_instructions.md &gt;}}</span>
<span id="cb2-712"><a href="#cb2-712"></a></span>
<span id="cb2-713"><a href="#cb2-713"></a>Under the column (field) headings in the Processed data panel table,</span>
<span id="cb2-714"><a href="#cb2-714"></a>there are input boxes that act as filters. The data presented in the</span>
<span id="cb2-715"><a href="#cb2-715"></a>table will be refined to just those cases that match the input string</span>
<span id="cb2-716"><a href="#cb2-716"></a>as it is being typed. It is possible to engage with any or all of</span>
<span id="cb2-717"><a href="#cb2-717"></a>these filters to help refine the search.</span>
<span id="cb2-718"><a href="#cb2-718"></a></span>
<span id="cb2-719"><a href="#cb2-719"></a>Under the table there is a **Download as csv** button. Via this</span>
<span id="cb2-720"><a href="#cb2-720"></a>button, you can download a comma separated text file version of the</span>
<span id="cb2-721"><a href="#cb2-721"></a>data in the table for further review in a spreadsheet of your choice.</span>
<span id="cb2-722"><a href="#cb2-722"></a>Once you click this button, you will be prompted to navigate to a</span>
<span id="cb2-723"><a href="#cb2-723"></a>suitable location to store the file.</span>
<span id="cb2-724"><a href="#cb2-724"></a></span>
<span id="cb2-725"><a href="#cb2-725"></a></span>
<span id="cb2-726"><a href="#cb2-726"></a><span class="fu">## Exploratory Data Analysis</span></span>
<span id="cb2-727"><a href="#cb2-727"></a></span>
<span id="cb2-728"><a href="#cb2-728"></a>The Exploratory Data Analysis page comprises four panels or subpages</span>
<span id="cb2-729"><a href="#cb2-729"></a>accessable by tabs at the top of the page and named &quot;Temporal&quot;,</span>
<span id="cb2-730"><a href="#cb2-730"></a>&quot;Temporal Type&quot;, &quot;Temporal Site&quot; and &quot;Spatial Type&quot;.</span>
<span id="cb2-731"><a href="#cb2-731"></a></span>
<span id="cb2-732"><a href="#cb2-732"></a><span class="fu">### Temporal</span></span>
<span id="cb2-733"><a href="#cb2-733"></a></span>
<span id="cb2-734"><a href="#cb2-734"></a>{{&lt; include ../md/eda_temporal.md &gt;}}</span>
<span id="cb2-735"><a href="#cb2-735"></a></span>
<span id="cb2-736"><a href="#cb2-736"></a><span class="fu">### Temporal Type</span></span>
<span id="cb2-737"><a href="#cb2-737"></a></span>
<span id="cb2-738"><a href="#cb2-738"></a>{{&lt; include ../md/eda_temporal_type.md &gt;}}</span>
<span id="cb2-739"><a href="#cb2-739"></a></span>
<span id="cb2-740"><a href="#cb2-740"></a><span class="fu">### Temporal Site</span></span>
<span id="cb2-741"><a href="#cb2-741"></a></span>
<span id="cb2-742"><a href="#cb2-742"></a>{{&lt; include ../md/eda_temporal_site.md &gt;}}</span>
<span id="cb2-743"><a href="#cb2-743"></a></span>
<span id="cb2-744"><a href="#cb2-744"></a><span class="fu">### Spatial Type</span></span>
<span id="cb2-745"><a href="#cb2-745"></a></span>
<span id="cb2-746"><a href="#cb2-746"></a>{{&lt; include ../md/eda_spatial_type.md &gt;}}</span>
<span id="cb2-747"><a href="#cb2-747"></a></span>
<span id="cb2-748"><a href="#cb2-748"></a><span class="fu">## Analysis</span></span>
<span id="cb2-749"><a href="#cb2-749"></a></span>
<span id="cb2-750"><a href="#cb2-750"></a>The Analysis page comprises three panels or subpages accessable by</span>
<span id="cb2-751"><a href="#cb2-751"></a>tabs at the top of the page and named &quot;Analysis overview&quot;, &quot;Model</span>
<span id="cb2-752"><a href="#cb2-752"></a>diagnostics&quot; and &quot;Analysis details&quot;.</span>
<span id="cb2-753"><a href="#cb2-753"></a></span>
<span id="cb2-754"><a href="#cb2-754"></a><span class="fu">### Analysis overview</span></span>
<span id="cb2-755"><a href="#cb2-755"></a></span>
<span id="cb2-756"><a href="#cb2-756"></a>The main feature of this panel is a table representing a very high</span>
<span id="cb2-757"><a href="#cb2-757"></a>level overview of the results conditional on spatial scale (Site,</span>
<span id="cb2-758"><a href="#cb2-758"></a>Zone, Area), Measurement Type (hydrocarbons or metals), Value Type</span>
<span id="cb2-759"><a href="#cb2-759"></a>(Unstandardised or Standardised), Normalisation (Standardisation)</span>
<span id="cb2-760"><a href="#cb2-760"></a>method, Focal Year and Contrast (each of which is controlled via a</span>
<span id="cb2-761"><a href="#cb2-761"></a>dropdown).</span>
<span id="cb2-762"><a href="#cb2-762"></a></span>
<span id="cb2-763"><a href="#cb2-763"></a>{{&lt; include ../md/temporal_analysis_overview_instructions.md &gt;}}</span>
<span id="cb2-764"><a href="#cb2-764"></a></span>
<span id="cb2-765"><a href="#cb2-765"></a>{{&lt; include ../md/temporal_analysis_instructions.md &gt;}}</span>
<span id="cb2-766"><a href="#cb2-766"></a></span>
<span id="cb2-767"><a href="#cb2-767"></a><span class="fu">### Model diagnostics</span></span>
<span id="cb2-768"><a href="#cb2-768"></a></span>
<span id="cb2-769"><a href="#cb2-769"></a>This panel displays a wide range of MCMC sampling and model validation</span>
<span id="cb2-770"><a href="#cb2-770"></a>diagnostics as well as simple model summaries.</span>
<span id="cb2-771"><a href="#cb2-771"></a></span>
<span id="cb2-772"><a href="#cb2-772"></a><span class="fu">#### Model Validation {#sec-validation}</span></span>
<span id="cb2-773"><a href="#cb2-773"></a></span>
<span id="cb2-774"><a href="#cb2-774"></a><span class="fu">##### Prior vs Posterior</span></span>
<span id="cb2-775"><a href="#cb2-775"></a></span>
<span id="cb2-776"><a href="#cb2-776"></a>{{&lt; include ../md/prior_vs_posterior.md &gt;}}</span>
<span id="cb2-777"><a href="#cb2-777"></a></span>
<span id="cb2-778"><a href="#cb2-778"></a><span class="fu">##### Traceplots</span></span>
<span id="cb2-779"><a href="#cb2-779"></a></span>
<span id="cb2-780"><a href="#cb2-780"></a>{{&lt; include ../md/traceplots.md &gt;}}</span>
<span id="cb2-781"><a href="#cb2-781"></a></span>
<span id="cb2-782"><a href="#cb2-782"></a><span class="fu">##### Autocorrelation plots</span></span>
<span id="cb2-783"><a href="#cb2-783"></a></span>
<span id="cb2-784"><a href="#cb2-784"></a>{{&lt; include ../md/acplots.md &gt;}}</span>
<span id="cb2-785"><a href="#cb2-785"></a></span>
<span id="cb2-786"><a href="#cb2-786"></a><span class="fu">##### Rhat plots</span></span>
<span id="cb2-787"><a href="#cb2-787"></a></span>
<span id="cb2-788"><a href="#cb2-788"></a>{{&lt; include ../md/rhatplots.md &gt;}}</span>
<span id="cb2-789"><a href="#cb2-789"></a></span>
<span id="cb2-790"><a href="#cb2-790"></a><span class="fu">##### Effective Sample Size</span></span>
<span id="cb2-791"><a href="#cb2-791"></a></span>
<span id="cb2-792"><a href="#cb2-792"></a>{{&lt; include ../md/essplots.md &gt;}}</span>
<span id="cb2-793"><a href="#cb2-793"></a></span>
<span id="cb2-794"><a href="#cb2-794"></a><span class="fu">##### Posterior Probability plots</span></span>
<span id="cb2-795"><a href="#cb2-795"></a></span>
<span id="cb2-796"><a href="#cb2-796"></a>{{&lt; include ../md/ppc.md &gt;}}</span>
<span id="cb2-797"><a href="#cb2-797"></a></span>
<span id="cb2-798"><a href="#cb2-798"></a><span class="fu">##### Simulated (DHARMa) residuals</span></span>
<span id="cb2-799"><a href="#cb2-799"></a></span>
<span id="cb2-800"><a href="#cb2-800"></a>{{&lt; include ../md/dharma.md &gt;}}</span>
<span id="cb2-801"><a href="#cb2-801"></a></span>
<span id="cb2-802"><a href="#cb2-802"></a><span class="fu">#### Model Summaries</span></span>
<span id="cb2-803"><a href="#cb2-803"></a></span>
<span id="cb2-804"><a href="#cb2-804"></a>{{&lt; include ../md/temporal_analysis_summary.md &gt;}}</span>
<span id="cb2-805"><a href="#cb2-805"></a></span>
<span id="cb2-806"><a href="#cb2-806"></a><span class="fu">### Analysis details</span></span>
<span id="cb2-807"><a href="#cb2-807"></a></span>
<span id="cb2-808"><a href="#cb2-808"></a>This panel has two sub-panels for displaying &quot;Modelled trends&quot; and</span>
<span id="cb2-809"><a href="#cb2-809"></a>&quot;Modelled effects&quot;.</span>
<span id="cb2-810"><a href="#cb2-810"></a></span>
<span id="cb2-811"><a href="#cb2-811"></a><span class="fu">#### Modelled trends</span></span>
<span id="cb2-812"><a href="#cb2-812"></a></span>
<span id="cb2-813"><a href="#cb2-813"></a>{{&lt; include ../md/temporal_analysis_trends.md &gt;}}</span>
<span id="cb2-814"><a href="#cb2-814"></a></span>
<span id="cb2-815"><a href="#cb2-815"></a><span class="fu">#### Modelled effects</span></span>
<span id="cb2-816"><a href="#cb2-816"></a></span>
<span id="cb2-817"><a href="#cb2-817"></a>{{&lt; include ../md/temporal_analysis_effects.md &gt;}}</span>
<span id="cb2-818"><a href="#cb2-818"></a></span>
<span id="cb2-819"><a href="#cb2-819"></a>{{&lt; fa thumbs-up &gt;}} </span>
<span id="cb2-820"><a href="#cb2-820"></a> </span></code></pre></div>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brms" class="csl-entry" role="listitem">
Bürkner, Paul-Christian. 2017. <span>“<span class="nocase">brms</span>: An <span>R</span> Package for <span>Bayesian</span> Multilevel Models Using <span>Stan</span>.”</span> <em>Journal of Statistical Software</em> 80 (1): 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>.
</div>
<div id="ref-Gelman-2007-2007" class="csl-entry" role="listitem">
Gelman, A., and J. Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge, UK: Cambridge University Press.
</div>
<div id="ref-DHARMa" class="csl-entry" role="listitem">
Hartig, Florian. 2022. <em>DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models</em>. <a href="https://CRAN.R-project.org/package=DHARMa">https://CRAN.R-project.org/package=DHARMa</a>.
</div>
<div id="ref-R-2024" class="csl-entry" role="listitem">
R Core Team. 2024. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
</div>
</section>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->

</body>

</html>
